{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_simple_v1_2_Merging.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "vww_wREStmg3",
        "i5LOmkkPu5ye",
        "csDRmfMrQVSy"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marvinmouroum/End-to-End-EEG-Classifier/blob/master/CNN_simple_v1_2_Merging.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXaFp2bbtQAN",
        "colab_type": "text"
      },
      "source": [
        "# A CNN to classify EEG data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cw3M85k3R9vA",
        "colab_type": "text"
      },
      "source": [
        "##Packages\n",
        "\n",
        "This section imports all the needed packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoJhN7K7tNSh",
        "colab_type": "code",
        "outputId": "8b0731df-c6bf-4591-91e7-4302dd6fbaf2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        }
      },
      "source": [
        "!pip install pydrive\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import roc_auc_score, precision_score, recall_score, accuracy_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "##### START OF ADDITION OF MY CODE\n",
        "\n",
        "import numpy as np\n",
        "import os.path\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import io\n",
        "from googleapiclient.http import MediaIoBaseDownload\n",
        "\n",
        "from scipy.io import loadmat\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io as sio\n",
        "import sys\n",
        "import pickle\n",
        "\n",
        "## PyTorch \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "from torch import Tensor\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "\n",
        "import math #for calculus\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pydrive\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/e0/0e64788e5dd58ce2d6934549676243dc69d982f198524be9b99e9c2a4fd5/PyDrive-1.3.1.tar.gz (987kB)\n",
            "\r\u001b[K     |▎                               | 10kB 14.1MB/s eta 0:00:01\r\u001b[K     |▋                               | 20kB 4.4MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 6.2MB/s eta 0:00:01\r\u001b[K     |█▎                              | 40kB 4.1MB/s eta 0:00:01\r\u001b[K     |█▋                              | 51kB 5.0MB/s eta 0:00:01\r\u001b[K     |██                              | 61kB 5.8MB/s eta 0:00:01\r\u001b[K     |██▎                             | 71kB 6.6MB/s eta 0:00:01\r\u001b[K     |██▋                             | 81kB 7.4MB/s eta 0:00:01\r\u001b[K     |███                             | 92kB 8.2MB/s eta 0:00:01\r\u001b[K     |███▎                            | 102kB 6.6MB/s eta 0:00:01\r\u001b[K     |███▋                            | 112kB 6.6MB/s eta 0:00:01\r\u001b[K     |████                            | 122kB 6.6MB/s eta 0:00:01\r\u001b[K     |████▎                           | 133kB 6.6MB/s eta 0:00:01\r\u001b[K     |████▋                           | 143kB 6.6MB/s eta 0:00:01\r\u001b[K     |█████                           | 153kB 6.6MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 163kB 6.6MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 174kB 6.6MB/s eta 0:00:01\r\u001b[K     |██████                          | 184kB 6.6MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 194kB 6.6MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 204kB 6.6MB/s eta 0:00:01\r\u001b[K     |███████                         | 215kB 6.6MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 225kB 6.6MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 235kB 6.6MB/s eta 0:00:01\r\u001b[K     |████████                        | 245kB 6.6MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 256kB 6.6MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 266kB 6.6MB/s eta 0:00:01\r\u001b[K     |█████████                       | 276kB 6.6MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 286kB 6.6MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 296kB 6.6MB/s eta 0:00:01\r\u001b[K     |██████████                      | 307kB 6.6MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 317kB 6.6MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 327kB 6.6MB/s eta 0:00:01\r\u001b[K     |███████████                     | 337kB 6.6MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 348kB 6.6MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 358kB 6.6MB/s eta 0:00:01\r\u001b[K     |████████████                    | 368kB 6.6MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 378kB 6.6MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 389kB 6.6MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 399kB 6.6MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 409kB 6.6MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 419kB 6.6MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 430kB 6.6MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 440kB 6.6MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 450kB 6.6MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 460kB 6.6MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 471kB 6.6MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 481kB 6.6MB/s eta 0:00:01\r\u001b[K     |████████████████                | 491kB 6.6MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 501kB 6.6MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 512kB 6.6MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 522kB 6.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 532kB 6.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 542kB 6.6MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 552kB 6.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 563kB 6.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 573kB 6.6MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 583kB 6.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 593kB 6.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 604kB 6.6MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 614kB 6.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 624kB 6.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 634kB 6.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 645kB 6.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 655kB 6.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 665kB 6.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 675kB 6.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 686kB 6.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 696kB 6.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 706kB 6.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 716kB 6.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 727kB 6.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 737kB 6.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 747kB 6.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 757kB 6.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 768kB 6.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 778kB 6.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 788kB 6.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 798kB 6.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 808kB 6.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 819kB 6.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 829kB 6.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 839kB 6.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 849kB 6.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 860kB 6.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 870kB 6.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 880kB 6.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 890kB 6.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 901kB 6.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 911kB 6.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 921kB 6.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 931kB 6.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 942kB 6.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 952kB 6.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 962kB 6.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 972kB 6.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 983kB 6.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 993kB 6.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.6/dist-packages (from pydrive) (1.7.9)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pydrive) (4.1.3)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.6/dist-packages (from pydrive) (3.13)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->pydrive) (3.0.0)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->pydrive) (0.11.3)\n",
            "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->pydrive) (1.12.0)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->pydrive) (1.4.2)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->pydrive) (0.0.3)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive) (4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive) (0.2.5)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive) (0.4.5)\n",
            "Requirement already satisfied: cachetools>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.2->pydrive) (3.1.1)\n",
            "Building wheels for collected packages: pydrive\n",
            "  Building wheel for pydrive (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/fa/d2/9a/d3b6b506c2da98289e5d417215ce34b696db856643bad779f4\n",
            "Successfully built pydrive\n",
            "Installing collected packages: pydrive\n",
            "Successfully installed pydrive-1.3.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vww_wREStmg3",
        "colab_type": "text"
      },
      "source": [
        "## Download the dataset\n",
        "\n",
        "This section contains all the functions in order to download the dataset and clean it,."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PH9EJ8jLEHtj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#np.set_printoptions(threshold=sys.maxsize)\n",
        "\n",
        "def load_file(file_id, content):\n",
        "  \n",
        "  print(\"loading file:\\n\",file_id)\n",
        "  \n",
        "  drive = GoogleDrive(gauth)\n",
        "  downloaded = drive.CreateFile({'id': file_id})\n",
        "  downloaded.GetContentFile(content)\n",
        "  \n",
        "  mat = sio.loadmat(content, squeeze_me=True, struct_as_record=False)\n",
        "\n",
        "  print(\"loaded dataset  ...\")\n",
        "\n",
        "  o=mat['o']\n",
        "  data=mat['o'].data\n",
        "  labels=mat['o'].marker\n",
        "\n",
        "  found =  []\n",
        "  \n",
        "  valid_data   = []\n",
        "  valid_labels = []\n",
        "\n",
        "  last_ind = 0\n",
        "  \n",
        "  print(\"\\ndataset contains \", data.shape ,\" entries\" )\n",
        "  \n",
        "  #check if there are invalid labels\n",
        "  if np.amax(labels) <= 3:\n",
        "    valid_data.append(data)\n",
        "    valid_labels.append(labels)\n",
        "  \n",
        "  #cut out chunks if there were invalid labels\n",
        "  else:\n",
        "    \n",
        "    #find the start\n",
        "    start = np.where((labels == 99))\n",
        "    clean_start = start[0][-1] + 1\n",
        "    \n",
        "    #find the breaks\n",
        "    breaks = np.where(labels == 91)\n",
        "    \n",
        "    #the very first break\n",
        "    if breaks[0].size != 0:\n",
        "      clean_breaks = [breaks[0][0]]\n",
        "    \n",
        "      #the intermediate break points\n",
        "      for i in range(len(breaks[0])-1):\n",
        "        if breaks[0][i] + 1 != breaks[0][i+1]:\n",
        "          clean_breaks.append(breaks[0][i])\n",
        "          clean_breaks.append(breaks[0][i+1])\n",
        "    \n",
        "      #the very last break point\n",
        "      if clean_breaks[-1] != breaks[-1][-1]:\n",
        "        clean_breaks.append(breaks[-1][-1])\n",
        "        \n",
        "      valid_data.append(data[clean_start:clean_breaks[0]])\n",
        "      valid_labels.append(labels[clean_start:clean_breaks[0]])\n",
        "      \n",
        "      for b in range(1,len(clean_breaks)-1,2):\n",
        "        valid_data.append(data[clean_breaks[b]+1:clean_breaks[b+1]])\n",
        "        valid_labels.append(labels[clean_breaks[b]+1:clean_breaks[b+1]])\n",
        "    else:\n",
        "      print(\"\\n !!! DATASET does NOT contain BREAK flags !!!\")\n",
        "    \n",
        "    #find the end\n",
        "    finish = np.where(labels == 92)\n",
        "    if finish[0].size != 0:\n",
        "      clean_finish = finish[0][0]\n",
        "    else:\n",
        "      clean_finish = labels.shape[0]\n",
        "        \n",
        "    \n",
        "    if breaks[0].size != 0:\n",
        "      valid_data.append(data[clean_breaks[-1]+1:clean_finish])\n",
        "      valid_labels.append(labels[clean_breaks[-1]+1:clean_finish])\n",
        "      \n",
        "    else:\n",
        "      valid_data.append(data[clean_start:clean_finish])\n",
        "      valid_labels.append(labels[clean_start:clean_finish])\n",
        "    \n",
        "\n",
        "  print(\"cleaned data  ...\")\n",
        "  \n",
        "  \n",
        "  newdata  = np.vstack(valid_data[0:len(valid_data)])\n",
        "  newlabels = np.hstack(valid_labels[0:len(valid_labels)])\n",
        "  \n",
        "  return (newdata,newlabels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MxBINl_L1d7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def combine_data(data,labels):\n",
        "  return np.hstack((data,labels.reshape((labels.shape[0],1))))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5LOmkkPu5ye",
        "colab_type": "text"
      },
      "source": [
        "##Video Stream Creation\n",
        "\n",
        "The imported data is converted into a 3D image. The electrodes are reconstructed assuming the head is a 3 dimensional ellipse.\n",
        "The calculation can be seen at:\n",
        "\n",
        "https://docs.google.com/spreadsheets/d/1TetDbE-HkAQGnVVIFbPGI4ugbktgdic_TFtsFyWEp0w/edit?usp=sharing\n",
        "\n",
        "The created files can be stored on the drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YyiQIYlvF4D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## video maker and data extractor\n",
        "\n",
        "pos=np.zeros((22,3), dtype=int)\n",
        "\n",
        "pos[0]=-2,10,0\n",
        "pos[1]=2,10,0\n",
        "pos[3]=-4,7,3\n",
        "pos[4]=4,7,3\n",
        "pos[5]=-5,0,4\n",
        "pos[6]=5,0,4\n",
        "pos[7]=-4,-7,3\n",
        "pos[8]=4,-7,3\n",
        "pos[9]=-2,-10,0\n",
        "pos[10]=2,-10,0\n",
        "pos[11]=-7,0,-5\n",
        "pos[12]=7,0,5\n",
        "pos[13]=-6,6,0\n",
        "pos[14]=6,6,0\n",
        "pos[15]=-7,0,0\n",
        "pos[16]=7,0,0\n",
        "pos[17]=-6,-6,0\n",
        "pos[18]=6,-6,0\n",
        "pos[19]=0,7,4\n",
        "pos[20]=0,0,6\n",
        "pos[21]=0,-7,4\n",
        "\n",
        "def make_3d_point(x,y,z,r,theta,phi):\n",
        "  return [\n",
        "      x + int(r * math.cos(theta)*math.sin(phi)),\n",
        "      y + int(r * math.sin(theta)*math.sin(phi)),\n",
        "      z + int(r * math.cos(phi))\n",
        "         ]\n",
        "\n",
        "def make_3d_data(image3d_data):\n",
        "  #creating the matrix\n",
        "  matrix = np.zeros([12,16,22]) # z,x,y\n",
        "  \n",
        "  #looping through data\n",
        "  for point in image3d_data:\n",
        "    \n",
        "    # creating the indice by shifting the coordinate system from the center to left corner\n",
        "    newpoint = [int(point[0]) + 7,int(point[1]) + 10,int(point[2]) + 5]\n",
        "    \n",
        "    if newpoint[0] >= matrix.shape[1] or newpoint[1] >= matrix.shape[2] or newpoint[2] >= matrix.shape[0] or newpoint[0] < 0 or newpoint[1] < 0 or newpoint[2] < 0:\n",
        "      #print(\"did not add point\")\n",
        "      #print(newpoint)\n",
        "      #print(matrix.shape)\n",
        "      continue\n",
        "\n",
        "    \n",
        "    #assigning the value\n",
        "    if abs(matrix[newpoint[2],newpoint[0],newpoint[1]]) <= abs(point[3]):\n",
        "        matrix[newpoint[2],newpoint[0],newpoint[1]] = int(point[3])\n",
        "\n",
        "    \n",
        "    #creating variables for creating a virtual sphere around poi\n",
        "    rho = [2,3] # distance (in pixels) that we are interpolating from poi\n",
        "    #theta  angle in radians    [0,2pi]\n",
        "    #phi    angle in radians    [0, pi]\n",
        "    \n",
        "    #creating new interpolated points\n",
        "    for r in rho:\n",
        "      for theta in range(0,360,45):\n",
        "        for phi in range(0,180,45):\n",
        "          p = make_3d_point(newpoint[0],newpoint[1],newpoint[2],r,math.radians(theta),math.radians(phi))\n",
        "          \n",
        "          #checking if its in bounds\n",
        "          if p[0] < matrix.shape[1] and p[1] < matrix.shape[2] and p[2] < matrix.shape[0]:\n",
        "            if abs(matrix[p[2],p[0],p[1]]) <= abs(point[3]/r):\n",
        "                matrix[p[2],p[0],p[1]] = int(point[3]/r)\n",
        "                     \n",
        "  return matrix\n",
        "\n",
        " \n",
        "def get_data(data,batch_size=256, test_batch_size=256):\n",
        "  \n",
        "\n",
        "  # Create train and validation splits\n",
        "  num_samples = len(data)\n",
        "  training_samples = int(num_samples*0.5+1)\n",
        "  validation_samples = num_samples - training_samples\n",
        "\n",
        "  #training_data, validation_data = torch.utils.data.random_split(data, [training_samples, validation_samples])\n",
        "  training_data   = data[0:training_samples]\n",
        "  validation_data = data[training_samples:-1]\n",
        "\n",
        "  # Initialize dataloaders\n",
        "  train_loader = torch.utils.data.DataLoader(training_data, batch_size, shuffle=False)\n",
        "  val_loader = torch.utils.data.DataLoader(validation_data, batch_size, shuffle=False)\n",
        "  #test_loader = torch.utils.data.DataLoader(test_data, test_batch_size, shuffle=False)\n",
        "  \n",
        "  print(\"loading data\")\n",
        "  \n",
        "  return train_loader, val_loader #, test_loader\n",
        "\n",
        "\n",
        "\n",
        "def get_video_data(loader):\n",
        "  \n",
        "  #print(\"get videos:\")\n",
        "\n",
        "  batchsize = loader.shape[0]\n",
        "  \n",
        "  training_data   = np.empty([batchsize,22,4])\n",
        "  \n",
        "  train_label = np.empty((batchsize))\n",
        "  \n",
        " # print(\"start collecting training data\")\n",
        "  \n",
        "  for it,timestep_data in enumerate(loader):\n",
        "    train3d = np.hstack((pos,timestep_data[0:22].reshape([22,1])))\n",
        "    training_data[it] = train3d\n",
        "    train_label[it] = timestep_data[22] \n",
        " \n",
        "    \n",
        "  return (training_data, train_label)\n",
        "  \n",
        "  \n",
        "  \n",
        "def make_video(video_data):\n",
        "  return make_3d_data(video_data)\n",
        "  \n",
        "  \n",
        "  \n",
        "def get_training_data(train,mean,std):\n",
        "\n",
        "  stream = np.zeros([train[0].shape[0],1,12,16,22])\n",
        "  \n",
        "  for i,image in enumerate(train[0]):\n",
        "    if i >= stream.shape[0]:\n",
        "      break\n",
        "    stream[i,0] = make_video(image)\n",
        "  \n",
        "  tensor = torch.from_numpy(stream)\n",
        "\n",
        "  tensor = (tensor - mean) / std\n",
        "  \n",
        "  return (tensor,train[1])\n",
        "\n",
        "def save_video(data,batch_size=256,path='VideoBatches/',mean=0,std=1):\n",
        "  \n",
        "  print(\"saving video files in VideoBatches with batch size: \",batch_size)\n",
        "  \n",
        "  train_loader, val_loader = get_data(data,batch_size)\n",
        "  \n",
        "  print(\"\\nsaving training data\\n\")\n",
        "  \n",
        "  for batch_idx, inputs in enumerate(train_loader):\n",
        "      \n",
        "    if batch_idx % 100 == 0:\n",
        "      print(\"saving batches: \" , batch_idx, \"-\", batch_idx+100)\n",
        "      \n",
        "    result_data = get_video_data(inputs)\n",
        "    result = get_training_data(result_data,mean,std)\n",
        "    \n",
        "    name = path + \"train_\" + str(batch_idx) + \".pickle\"\n",
        "    \n",
        "    with open(name, 'wb') as f:\n",
        "      pickle.dump([result[0], result[1]], f)\n",
        "      \n",
        "  print(\"\\nsaving validation data\\n\")\n",
        "  \n",
        "  for batch_idx, inputs in enumerate(train_loader):\n",
        "\n",
        "    if batch_idx % 100 == 0:\n",
        "      print(\"saving batches: \" , batch_idx, \"-\", batch_idx+100)\n",
        "      \n",
        "    result_data = get_video_data(inputs)\n",
        "    result = get_training_data(result_data,mean,std)\n",
        "    \n",
        "    name = path + \"val_\" + str(batch_idx) + \".pickle\"\n",
        "    \n",
        "    with open(name, 'wb') as f:\n",
        "      pickle.dump([result[0], result[1]], f)\n",
        "    \n",
        "    \n",
        "    \n",
        "  \n",
        "  \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JV7QyjoVN2nm",
        "colab_type": "code",
        "outputId": "6d2790f3-dd09-44d9-cc38-fa1087c41e8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive as gdrive\n",
        "\n",
        "gdrive.mount('/content/gdrive')\n",
        "root_path = 'gdrive/My Drive/DeepLearning/VideoBatches/subject01/'\n",
        "\n",
        "def save_video_to_drive(_id,file_id,file_name,directory,batch_size=256):\n",
        "  \n",
        "  root_path = 'gdrive/My\\ Drive/' + directory\n",
        "  \n",
        "  save_dir = root_path + 'subject' + str(_id)\n",
        "  \n",
        "  print(\"creating directory\\n\",save_dir)\n",
        "  \n",
        "  !mkdir $save_dir\n",
        "  \n",
        "  root_path = 'gdrive/My Drive/' + directory\n",
        "  \n",
        "  save_dir = root_path + 'subject' + str(_id)\n",
        "  \n",
        "  _dir = save_dir + '/'\n",
        "  \n",
        "  print(\"creating data for \" , file_name)\n",
        "  \n",
        "  cleaned_data = load_file(file_id,file_name)\n",
        "  \n",
        "  mean = cleaned_data[0].mean()\n",
        "  std  = cleaned_data[0].std() \n",
        "  \n",
        "  data = combine_data(cleaned_data[0],cleaned_data[1])\n",
        "  \n",
        "  save_video(data,batch_size,_dir,mean,std)\n",
        "  \n",
        "  print(\"Done\")\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AFrzgz1AdIj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twp5mzGalaLc",
        "colab_type": "code",
        "outputId": "0b5caa9b-9d52-4556-c336-4ec32696c939",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1632
        }
      },
      "source": [
        "!ls\n",
        "\n",
        "file_ids = ['1FrXb6rTyqpE5SmNtP8HhygDmFf9Lw896','19cjwiSqdKK-bioqQK_vbptbYSYJGuop1','10zeWbMn_j8pl_sZrlzvNWSumedr2etca','1TqewoCjjRXZpEIxL_23ZQ8MP4L0RofZx','1-2pj-scr75Y3iI_gHUS17WXHpgQzT6_f','167lgnY18GUIJiy9mIungxtSk-LSTOoar','1v8RaiTYN82BUJuOXIeJH5YvUWP42ZyNc','15AdOQt4rNEPe2WKjVfqwOJluCQHQCW2E','1bIDO5pDIvs5KRNmew-weFCN8rCaHU8v5','1WU2U86bPxXCjc-laHca1jedq_PqIDKiL','1hKxhB6paBy_MZVMGlcmWFFF8XO3D_1m4','1U7iQryUNXfVhls6tjDz81yK7UrAUTKGB','16OHZ3foYB8JKvC1TcqO6gYvwPBnYqpci','11VRncgCXRjBktla-cNOE2W-w8stywB6u','1YMq3v0FmWwWB519dB8AvybeXkHPcHMFF','1ilT5ZXa_6rpGf7pb02rX0-PuMbuSzIlI','1pDy1AI4Z9mLRJ6gBkzHBoc7qI3TKMErP']\n",
        "\n",
        "number = len(file_ids)\n",
        "\n",
        "for i in range(6,int(number/2)):\n",
        "  auth.authenticate_user()\n",
        "  gauth = GoogleAuth()\n",
        "  gauth.credentials = GoogleCredentials.get_application_default()\n",
        "  save_video_to_drive(str(i),file_ids[i],'train'+ str(i)+ '.mat','DeepLearning/VideoBatches/',256)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "adc.json  gdrive  sample_data  train7.mat\n",
            "creating directory\n",
            " gdrive/My\\ Drive/DeepLearning/VideoBatches/subject6\n",
            "mkdir: cannot create directory ‘gdrive/My Drive/DeepLearning/VideoBatches/subject6’: File exists\n",
            "creating data for  train6.mat\n",
            "loading file:\n",
            " 1v8RaiTYN82BUJuOXIeJH5YvUWP42ZyNc\n",
            "loaded dataset  ...\n",
            "\n",
            "dataset contains  (667600, 22)  entries\n",
            "cleaned data  ...\n",
            "saving video files in VideoBatches with batch size:  256\n",
            "loading data\n",
            "\n",
            "saving training data\n",
            "\n",
            "saving batches:  0 - 100\n",
            "saving batches:  100 - 200\n",
            "saving batches:  200 - 300\n",
            "saving batches:  300 - 400\n",
            "saving batches:  400 - 500\n",
            "saving batches:  500 - 600\n",
            "saving batches:  600 - 700\n",
            "saving batches:  700 - 800\n",
            "saving batches:  800 - 900\n",
            "saving batches:  900 - 1000\n",
            "saving batches:  1000 - 1100\n",
            "saving batches:  1100 - 1200\n",
            "saving batches:  1200 - 1300\n",
            "saving batches:  1300 - 1400\n",
            "\n",
            "saving validation data\n",
            "\n",
            "saving batches:  0 - 100\n",
            "saving batches:  100 - 200\n",
            "saving batches:  200 - 300\n",
            "saving batches:  300 - 400\n",
            "saving batches:  400 - 500\n",
            "saving batches:  500 - 600\n",
            "saving batches:  600 - 700\n",
            "saving batches:  700 - 800\n",
            "saving batches:  800 - 900\n",
            "saving batches:  900 - 1000\n",
            "saving batches:  1000 - 1100\n",
            "saving batches:  1100 - 1200\n",
            "saving batches:  1200 - 1300\n",
            "saving batches:  1300 - 1400\n",
            "Done\n",
            "creating directory\n",
            " gdrive/My\\ Drive/DeepLearning/VideoBatches/subject7\n",
            "mkdir: cannot create directory ‘gdrive/My Drive/DeepLearning/VideoBatches/subject7’: File exists\n",
            "creating data for  train7.mat\n",
            "loading file:\n",
            " 15AdOQt4rNEPe2WKjVfqwOJluCQHQCW2E\n",
            "loaded dataset  ...\n",
            "\n",
            "dataset contains  (669400, 22)  entries\n",
            "\n",
            " !!! DATASET does NOT contain BREAK flags !!!\n",
            "cleaned data  ...\n",
            "saving video files in VideoBatches with batch size:  256\n",
            "loading data\n",
            "\n",
            "saving training data\n",
            "\n",
            "saving batches:  0 - 100\n",
            "saving batches:  100 - 200\n",
            "saving batches:  200 - 300\n",
            "saving batches:  300 - 400\n",
            "saving batches:  400 - 500\n",
            "saving batches:  500 - 600\n",
            "saving batches:  600 - 700\n",
            "saving batches:  700 - 800\n",
            "saving batches:  800 - 900\n",
            "saving batches:  900 - 1000\n",
            "saving batches:  1000 - 1100\n",
            "saving batches:  1100 - 1200\n",
            "saving batches:  1200 - 1300\n",
            "\n",
            "saving validation data\n",
            "\n",
            "saving batches:  0 - 100\n",
            "saving batches:  100 - 200\n",
            "saving batches:  200 - 300\n",
            "saving batches:  300 - 400\n",
            "saving batches:  400 - 500\n",
            "saving batches:  500 - 600\n",
            "saving batches:  600 - 700\n",
            "saving batches:  700 - 800\n",
            "saving batches:  800 - 900\n",
            "saving batches:  900 - 1000\n",
            "saving batches:  1000 - 1100\n",
            "saving batches:  1100 - 1200\n",
            "saving batches:  1200 - 1300\n",
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mp2R0GMdjNww",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csDRmfMrQVSy",
        "colab_type": "text"
      },
      "source": [
        "## Visualize 3D Images\n",
        "\n",
        "This section contains the functions in order create a 3d scatter plot and save it on the drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjUeDzZiQbBj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "      \n",
        "def make_plots(name,start=0,end=1):\n",
        "  \n",
        "  with open(name, 'rb') as f:\n",
        "    result = pickle.load(f)\n",
        "    \n",
        "  points = []\n",
        "  cocos  = []\n",
        "\n",
        "  for i in range(start,end):\n",
        "\n",
        "    point = np.zeros([1,3])\n",
        "    coco = np.chararray([1])\n",
        "\n",
        "    coco[0] = '0'\n",
        "\n",
        "    for z in range(result[0][i,0].shape[0]):\n",
        "      for x in range(result[0][i,0,z].shape[0]):\n",
        "        for y in range(result[0][i,0,z,x].shape[0]):\n",
        "          if abs(result[0][i,0,z,x,y]) > 0:\n",
        "            raw   = result[0][i,0,z,x,y]\n",
        "            clean = (raw + 1)/2\n",
        "            point = np.append(point,[[x,y,z]],axis=0)\n",
        "            coco = np.append(coco,['%.5f' % clean], axis=0)\n",
        "        \n",
        "\n",
        "    coco = coco[1:coco.shape[0]].reshape([coco.shape[0]-1])\n",
        "    point = point[1:point.shape[0]].reshape([point.shape[0]-1,3])\n",
        "    \n",
        "    points.append(point)\n",
        "    cocos.append(coco)\n",
        "\n",
        "    \n",
        "    \n",
        "    \n",
        "  return (points, cocos, result[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zXV6raWQbp4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import axes3d\n",
        "\n",
        "import sys\n",
        "np.set_printoptions(threshold=sys.maxsize)\n",
        "\n",
        "\n",
        "def save_3d_plots(save_to,data_path,start,end,plot=False):\n",
        "\n",
        "  video = make_plots(data_path,start,end)\n",
        "\n",
        "\n",
        "  for i, frame in enumerate(video[0]):\n",
        "    print(\"frame \",i)\n",
        "  \n",
        "    point = frame\n",
        "    coco  = video[1][i]\n",
        "\n",
        "    data = np.empty([point.shape[0],3,1])\n",
        "  \n",
        "    data[:,0,0] = point[:,0]\n",
        "    data[:,1,0] = point[:,1]\n",
        "    data[:,2,0] = point[:,2]\n",
        "\n",
        "    colors = coco\n",
        "\n",
        "    groups = (\"coffee\", \"tea\", \"water\") \n",
        "\n",
        "    # Create plot\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    ax = fig.gca(projection='3d')\n",
        "\n",
        "    print(\"label is -> \" , video[2][i])\n",
        "    \n",
        "    if video[2][i] == 1:\n",
        "      ax.set_facecolor('xkcd:mint green')\n",
        "    elif video[2][i] == 2:\n",
        "      ax.set_facecolor('xkcd:salmon')\n",
        "    elif video[2][i] == 3:\n",
        "      ax.set_facecolor('xkcd:blue')\n",
        "\n",
        "    for data, color in zip(data, colors):\n",
        "      x, y, z = data\n",
        "      ax.scatter(x, y, z, alpha=0.8, c=color, edgecolors='none', s=30)\n",
        "\n",
        "    plt.title('Matplot 3d scatter plot')\n",
        "    plt.legend(loc=2)\n",
        "    \n",
        "    if plot == True:\n",
        "      plt.show()\n",
        "  \n",
        "    image_name = save_to + \"brain_\" + str(start + i) + \".jpg\"\n",
        "    plt.savefig(image_name)\n",
        "    \n",
        "    if plot == False:\n",
        "      plt.close(fig)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQSRPLaxMgcb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "root_path = 'gdrive/My Drive/DeepLearning/'\n",
        "\n",
        "path = root_path + \"VideoBatches/subject0/train_1178.pickle\"\n",
        "save = root_path + \"BrainImage/\"\n",
        "\n",
        "!mkdir $save\n",
        "\n",
        "start = 0\n",
        "end   = 254\n",
        "\n",
        "save_3d_plots(save,path,start,end,plot=False)\n",
        "\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtP2QmbcvfkI",
        "colab_type": "text"
      },
      "source": [
        "## CNN\n",
        "\n",
        "To-Do: \n",
        "- Find a good architecture\n",
        "- Test with more data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rWMFiUWPQYl",
        "colab_type": "code",
        "outputId": "c4df83dc-0483-4ed7-be1b-47214bbe44d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        }
      },
      "source": [
        "from google.colab import drive as gdrive\n",
        "\n",
        "gdrive.mount('/content/gdrive')\n",
        "root_path = 'gdrive/My Drive/DeepLearning/VideoBatches/'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAS-jM8voJLa",
        "colab_type": "code",
        "outputId": "f1ef8bdf-9e7f-4682-a76c-d3d168cb16e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gdrive\tsample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eMviRKhYf3G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SimpleLSTM(nn.Module):\n",
        "    def __init__(self, input_size=22, hidden_size=22, output_size=4,\n",
        "                 device='cuda:0'):\n",
        "        super(SimpleLSTM, self).__init__()\n",
        "        \n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        \n",
        "        \n",
        "        self.i2h = nn.LSTM(input_size, 22,hidden_size,bias=False,batch_first=False)\n",
        "        self.i2o = nn.Linear(hidden_size, output_size)\n",
        "        \n",
        "    def forward(self, input, hidden=None, cell=None, device='cuda:0'):\n",
        "        outputs = []\n",
        "        \n",
        "        if hidden==None:\n",
        "          #print(\"hidden input hidden:\",input.shape[1])\n",
        "          hidden = self.init_hidden(input.shape[1]) \n",
        "          hidden.to(device)\n",
        "        \n",
        "        if cell==None:\n",
        "          #print(\"hidden input cell:\",input.shape[1])\n",
        "          cell = self.init_hidden(input.shape[1])\n",
        "          cell.to(device)\n",
        "        \n",
        "        output, (_,_)= self.i2h(input, (hidden,cell))\n",
        "        \n",
        "        output = self.i2o(output)\n",
        "        \n",
        "        return F.softmax(output,dim=2)\n",
        "\n",
        "    def init_hidden(self,shape=1, device='cuda:0'):\n",
        "        return torch.zeros(22, shape, self.hidden_size).to(device)\n",
        "      \n",
        "    def init_cell(self,shape=1, device='cuda:0'):\n",
        "        return torch.zeros(22, shape, self.hidden_size).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkY09U20nqnj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class eeg_shallow_CNN(nn.Module):\n",
        "    \n",
        "    def __init__(self,name='shallow_network'):\n",
        "        super(eeg_shallow_CNN, self).__init__()\n",
        "        self.T = 120\n",
        "        \n",
        "        self.name = name\n",
        "        \n",
        "        self.training_epochs = 0\n",
        "        \n",
        "        self.lr_history       = []\n",
        "        self.accuracy_history = []\n",
        "        self.loss_history     = []\n",
        "        \n",
        "        self.conv1 = nn.Conv3d(1,6,(2,2,2), stride=(1,1,1),padding=(2,2,2),dilation=1)\n",
        "        self.batchnorm1 = nn.BatchNorm3d(6)\n",
        "        \n",
        "        self.conv2 = nn.Conv3d(6,12,(2,2,2), stride=(1,1,1),padding=(1,1,1),dilation=1)\n",
        "        self.batchnorm2 = nn.BatchNorm3d(12)\n",
        "        \n",
        "        self.pooling1 = nn.MaxPool3d((2,2,2), stride=(1,1,1), dilation=1)\n",
        "        \n",
        "        #layer after max pool\n",
        "        self.conv3 = nn.Conv3d(12,17,(2,2,2), stride=(1,1,1),padding=(1,1,0),dilation=1)\n",
        "        self.batchnorm3 = nn.BatchNorm3d(17)\n",
        "        \n",
        "        self.conv4 = nn.Conv3d(17,22,(1,1,1), stride=(1,1,1),dilation=1)\n",
        "        self.batchnorm4 = nn.BatchNorm3d(22)\n",
        "        \n",
        "        self.pooling2 = nn.MaxPool3d((2,3,2), stride=(1,1,1), dilation=1)\n",
        "        \n",
        "        #Layer after next max pool \n",
        "        self.conv5 = nn.Conv3d(22,11,(3,4,4), stride=(2,2,1),padding=(2,2,2),dilation=1)\n",
        "        self.batchnorm5 = nn.BatchNorm3d(11)\n",
        "        self.conv6 = nn.Conv3d(11,1,(3,4,1), stride=(2,2,1),padding=(0,1,0),dilation=1)\n",
        "        self.batchnorm6 = nn.BatchNorm3d(1)\n",
        "        \n",
        "        self.pooling3 = nn.MaxPool3d((4,5,3), stride=(2,1,1), dilation=1)\n",
        "        \n",
        "        #fully connected at the end\n",
        "        self.fc1 = torch.nn.Linear(in_features=1 * 1 * 22, out_features=11)\n",
        "        self.fc2 = torch.nn.Linear(in_features=11, out_features=4)\n",
        "        \n",
        "        self.rnn = SimpleLSTM()\n",
        "        \n",
        "        # robust weight initialization\n",
        "        torch.nn.init.xavier_normal_(self.conv1.weight)\n",
        "        torch.nn.init.xavier_normal_(self.conv2.weight)\n",
        "        torch.nn.init.xavier_normal_(self.conv3.weight)\n",
        "        torch.nn.init.xavier_normal_(self.conv4.weight)\n",
        "        torch.nn.init.xavier_normal_(self.conv5.weight)\n",
        "        torch.nn.init.xavier_normal_(self.conv6.weight)\n",
        "\n",
        "        torch.nn.init.xavier_normal_(self.fc1.weight)\n",
        "        torch.nn.init.xavier_normal_(self.fc2.weight)\n",
        "      \n",
        "\n",
        "        \n",
        "    def forward(self, x):\n",
        "\n",
        "        # first set of CNNs and then a max pool\n",
        "        x = self.conv1(x)\n",
        "        x = self.batchnorm1(x)\n",
        "        x = F.relu(x)\n",
        "        \n",
        "        x = self.conv2(x)\n",
        "        x = self.batchnorm2(x)\n",
        "        x = F.relu(x)\n",
        "        \n",
        "        x = self.pooling1(x)\n",
        "        \n",
        "        # second set\n",
        "        \n",
        "        x = self.conv3(x)\n",
        "        x = self.batchnorm3(x)\n",
        "        x = F.relu(x)\n",
        "        \n",
        "        x = self.conv4(x)\n",
        "        x = self.batchnorm4(x)\n",
        "        x = F.relu(x)\n",
        "        \n",
        "        x = self.pooling2(x)\n",
        "        \n",
        "        # set 3\n",
        "        \n",
        "        x = self.conv5(x)\n",
        "        x = self.batchnorm5(x)\n",
        "        x = F.relu(x)\n",
        "        \n",
        "        x = self.conv6(x)\n",
        "        x = self.batchnorm6(x)\n",
        "        x = F.relu(x)\n",
        "        \n",
        "        x = self.pooling3(x)\n",
        "        \n",
        "        batch_size, timesteps, C, H, W = x.size()\n",
        "        \n",
        "        x = x.view(batch_size, timesteps, W)\n",
        "        \n",
        "        return self.rnn(x)\n",
        "      \n",
        "    def save(self,root='gdrive/My\\ Drive/DeepLearning/Nets'):\n",
        "      !mkdir $root\n",
        "  \n",
        "      path = root.replace('\\\\','') + '/' + self.name + '.pickle'\n",
        "    \n",
        "      print(\"\\nsaving network under:\\n\",path)\n",
        "      \n",
        "      with open(path, 'wb') as f:\n",
        "        pickle.dump(self, f)\n",
        "        \n",
        "    def next_epoch(self,lr,acc_hist,loss_hist):\n",
        "      self.training_epochs += 1\n",
        "      self.lr_history.append(lr)\n",
        "      self.accuracy_history.append(acc_hist)\n",
        "      self.loss_history.append(loss_hist)\n",
        "    \n",
        "    def reset(self):\n",
        "      self.training_epochs = 0\n",
        "      self.lr_history = []\n",
        "      self.accuracy_history = []\n",
        "      self.loss_history = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKYm2xQLnq65",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_cost_function():\n",
        "    \n",
        "  cost_function = torch.nn.CrossEntropyLoss()\n",
        "  return cost_function\n",
        "    \n",
        "def get_optimizer(net, lr, wd, momentum):\n",
        "  #optimizer = torch.optim.SGD(net.parameters(), lr=lr, weight_decay=wd, momentum=momentum)\n",
        "  optimizer = torch.optim.Adam(net.parameters(), lr=lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=wd, amsgrad=False)\n",
        "  return optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtnqGj-3pdp0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#the ids for the folders in drive containing the pre processed data for every subject\n",
        "# subject 7 is not valid and missing\n",
        "# makes the training process more controllable\n",
        "\n",
        "training_folder_ids = ['1VX1qChrP0CsxumaAqZ66HUYZdgO65jre', #subject 0\n",
        "                       '1rOHo2Wu_Rfng1-nYVWy5YdeWH1jlxwQ4', #subject 1\n",
        "                       '188Wrvwk6M1KDB1ydg5FSWk3PBhnyrX5D', #subject 2\n",
        "                       '1U4XYEHLvIrtLMSJxbb-lt6clpjHx7o4-', #subject 3\n",
        "                       '1k0GTcLtTuZVGEPg0zW_QrL3o7HiLH7Sy', #subject 4\n",
        "                       '10D64lIdH7kxlxHUU5h6BWf40a4sLOF07', #subject 5\n",
        "                       '1brTnGtGzH5v10J_OfWJnL4D0l0i8hU9x', #subject 6\n",
        "                       '',\n",
        "                       '1ouD338EaxRTDqvK5m6LxgGQQK7iVAvwt', #subject 8 !!!\n",
        "                       '1TG49iJEtNBxSlqxtkOqKOJ2xClDX8GYq', #subject 9\n",
        "                       '13-RkhEErzwuuNvMKwQrlvLlJ97B-IQY5', #subject 10\n",
        "                       '1gaoPwQqketkoh6tk_PuxoEK1x5TzC8wp', #subject 11\n",
        "                       '1IXOlZY_lAkc9p_BYho7bo5cu82nvsGOp', #subject 12\n",
        "                       '13o2xxTWU56rPF-cTMsmOrWlCisrCgcPe', #subject 13\n",
        "                       '1cASt5iWOe_GV0y5TaV7K7o6IzJqNkeHl', #subject 14\n",
        "                       '12i7kMF7vJQvaZ37MD6bCm6Rui8OW203Y', #subject 15\n",
        "                       '1TclA0tmxgnEhy8pquMYfPwWeEmFWiB7U'] #subject 16"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kV5lS6oDtp7U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "from pydrive.drive import GoogleDrive\n",
        "\n",
        "def get_preprocessed_data_count_from(subject):\n",
        "  \n",
        "  auth.authenticate_user()\n",
        "  gauth = GoogleAuth()\n",
        "  gauth.credentials = GoogleCredentials.get_application_default()\n",
        "  drive = GoogleDrive(gauth)\n",
        "\n",
        "  file_list = drive.ListFile({'q': \"'\" + training_folder_ids[subject] + \"' in parents and trashed=false\"}).GetList()\n",
        "\n",
        "  return len(file_list)\n",
        "\n",
        "#get_preprocessed_data_count_from(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CILv1uophoHm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(net, data_type, cost_function, device='cuda:0', start=0,end=16):\n",
        "  samples = 0.\n",
        "  sequences = 0.\n",
        "  cumulative_loss = 0.\n",
        "  cumulative_accuracy = 0.\n",
        "  cumulative_seq_accuracy = 0.\n",
        "\n",
        "  seq = 200\n",
        "\n",
        "  net.eval() # Strictly needed if network contains layers which has different behaviours between train and test\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    \n",
        "    for subject in range(start,end,1):\n",
        "      if subject == 7:\n",
        "        continue\n",
        "    \n",
        "      print(\"\\nTesting on subject \", subject)\n",
        "      \n",
        "      length = int(get_preprocessed_data_count_from(subject)/2)\n",
        "    \n",
        "      skipped = 0\n",
        "      kept    = 0\n",
        "    \n",
        "      for i in range(length-1-4):\n",
        "        \n",
        "        if i % 300 == 0:\n",
        "          print(\"on test batch: \" , i, \"-\", i+300)\n",
        "       \n",
        "        results = []\n",
        "        \n",
        "        tmp_skipped = 0\n",
        "        j = 0\n",
        "        \n",
        "        while j < 4 + tmp_skipped and i + 4 + tmp_skipped < length:\n",
        "          j += 1\n",
        "          \n",
        "          name = root_path + \"subject\" + str(subject) + \"/\" + data_type + \"_\" + str(i+j) + \".pickle\"\n",
        "      \n",
        "          with open(name, 'rb') as f:\n",
        "            subresult = pickle.load(f)\n",
        "\n",
        "          clean_sub = clean_seq(subresult[0],subresult[1],seq)\n",
        "          \n",
        "          if clean_sub == None:\n",
        "            skipped += 1\n",
        "            tmp_skipped += 1\n",
        "            #print(\"skipped a sequence\")\n",
        "            continue\n",
        "          else:\n",
        "            kept += 1\n",
        "            \n",
        "          \n",
        "          sub = (clean_sub[0],clean_sub[1])\n",
        "          results.append(sub)\n",
        "          \n",
        "          \n",
        " \n",
        "        if len(results) < 4:\n",
        "          print(\"not enough data left for rnn -> next epoch pls\")\n",
        "          continue\n",
        "        \n",
        "        \n",
        "        t_data   = Variable( torch.from_numpy(\n",
        "                                         np.concatenate( \n",
        "                                        (np.concatenate((results[0][0],results[1][0]),  axis=0) , \n",
        "                                         np.concatenate((results[2][0],results[3][0]),  axis=0)) ,\n",
        "                                        axis=0)\n",
        "                              ),\n",
        "                            requires_grad=True\n",
        "                                   )\n",
        "        \n",
        "        t_labels = np.concatenate((np.concatenate((results[0][1],results[1][1]), axis=None) , \n",
        "                                   np.concatenate((results[2][1],results[3][1]), axis=None)) ,\n",
        "                                  axis = None)\n",
        "                 \n",
        "        #print(t_data)\n",
        "        \n",
        "        if t_data.shape[0] < 4*seq:\n",
        "          print(\"sequence length too short\")\n",
        "          print(t_data.shape)\n",
        "          continue\n",
        "        \n",
        "        max_v = t_labels.max()\n",
        "      \n",
        "        if max_v > 3:\n",
        "          #print(\"\\nlabels were not correct\" , max_v , \", skipped batch: \" , i)\n",
        "          continue\n",
        "      \n",
        "        #zero mean and normalize the data\n",
        "        mean = t_data.mean()\n",
        "        std  = t_data.std()\n",
        "      \n",
        "        t_data = (t_data - mean)/std\n",
        "      \n",
        "        # Load data into GPU\n",
        "        inputs = t_data.float().to(device)\n",
        "    \n",
        "        targets = torch.LongTensor(t_labels.reshape((4,seq))[0:4,seq-1]).to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        #outputs = net(inputs).reshape([inputs.shape[0],4])\n",
        "        outputs = net(inputs).reshape([4,seq,4])[0:4,seq-1]\n",
        "      \n",
        "      \n",
        "        # Apply the loss\n",
        "        loss = cost_function(outputs, targets)\n",
        "\n",
        "\n",
        "        #print(\"loss\",loss)\n",
        "\n",
        "        samples+=4\n",
        "        \n",
        "        sequences+=4\n",
        "        \n",
        "        cumulative_loss += loss.item() # Note: the .item() is needed to extract scalars from tensors\n",
        "        \n",
        "        _, predicted = outputs.max(1)\n",
        "      \n",
        "        #print(\"outputs from rnn -> \", rnnOutputs.shape)\n",
        "        \n",
        "        #seq_results = outputs.view(4,seq,outputs.shape[1])[0:4 , seq-1] # (4,4)\n",
        "        \n",
        "        #print(\"last outputs ->\", seq_results, seq_results.shape)\n",
        "        \n",
        "        #_, predicted_s = seq_results.max(1)\n",
        "        \n",
        "        #print(\"max predicted value\",predicted_s)\n",
        "        \n",
        "        #seq_targets   = targets.view(4,seq)[0:4,seq-1]\n",
        "        \n",
        "        #print(\"seq_targets -> \", seq_targets)\n",
        "        \n",
        "        #cumulative_seq_accuracy += predicted_s.eq(seq_targets).sum().item()\n",
        "        \n",
        "        cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "      \n",
        "      print(\"Accuracy after subject evaluation -> \" , cumulative_accuracy/samples*100)\n",
        "      #print(\"Sequence Accuracy after subject evaluation -> \" , cumulative_seq_accuracy/sequences*100)\n",
        "      \n",
        "      print( int(100*skipped/(skipped+kept)) ,\"% of the data was ignored due to sequence cleaning\")\n",
        "  \n",
        "  \n",
        "  return cumulative_loss/samples, cumulative_accuracy/samples*100, cumulative_accuracy/samples*100# cumulative_seq_accuracy/sequences*100\n",
        "\n",
        "\n",
        "def train(net,data_type,optimizer,cost_function, device='cuda:0', start=0,end=16):\n",
        "  samples = 0.\n",
        "  sequences = 0.\n",
        "  cumulative_loss = 0.\n",
        "  cumulative_accuracy = 0.\n",
        "  cumulative_seq_accuracy = 0.\n",
        "\n",
        "  seq = 200\n",
        "  \n",
        "  net.train() # Strictly needed if network contains layers which has different behaviours between train and test\n",
        "  \n",
        "  for subject in range(start,end,1):\n",
        "    if subject == 7:\n",
        "      continue\n",
        "    \n",
        "    print(\"\\nTraining on subject \", subject)\n",
        "      \n",
        "    length = int(get_preprocessed_data_count_from(subject)/2)\n",
        "    \n",
        "    skipped = 0\n",
        "    kept    = 0\n",
        "    \n",
        "    for i in range(length-1-4):\n",
        "        \n",
        "      if i % 300 == 0:\n",
        "        print(\"on train batch: \" , i, \"-\", i+300)\n",
        "       \n",
        "      results = []\n",
        "        \n",
        "      tmp_skipped = 0\n",
        "      j = 0\n",
        "        \n",
        "      while j < 4 + tmp_skipped and i + 4 + tmp_skipped < length:\n",
        "        j += 1\n",
        "          \n",
        "        name = root_path + \"subject\" + str(subject) + \"/\" + data_type + \"_\" + str(i+j) + \".pickle\"\n",
        "      \n",
        "        with open(name, 'rb') as f:\n",
        "          subresult = pickle.load(f)\n",
        "\n",
        "        clean_sub = clean_seq(subresult[0],subresult[1],seq)\n",
        "          \n",
        "        if clean_sub == None:\n",
        "          skipped += 1\n",
        "          tmp_skipped += 1\n",
        "            #print(\"skipped a sequence\")\n",
        "          continue\n",
        "        else:\n",
        "          kept += 1\n",
        "            \n",
        "          \n",
        "        sub = (clean_sub[0],clean_sub[1])\n",
        "        results.append(sub)\n",
        "          \n",
        "          \n",
        " \n",
        "      if len(results) < 4:\n",
        "        print(\"not enough data left for rnn -> next epoch pls\")\n",
        "        continue\n",
        "        \n",
        "        \n",
        "      t_data   = Variable( torch.from_numpy(\n",
        "                                         np.concatenate( \n",
        "                                        (np.concatenate((results[0][0],results[1][0]),  axis=0) , \n",
        "                                         np.concatenate((results[2][0],results[3][0]),  axis=0)) ,\n",
        "                                        axis=0)\n",
        "                              ),\n",
        "                            requires_grad=True\n",
        "                                   )\n",
        "        \n",
        "      t_labels = np.concatenate((np.concatenate((results[0][1],results[1][1]), axis=None) , \n",
        "                                   np.concatenate((results[2][1],results[3][1]), axis=None)) ,\n",
        "                                  axis = None)\n",
        "                 \n",
        "        #print(t_data)\n",
        "        \n",
        "      if t_data.shape[0] < 4*seq:\n",
        "        print(\"sequence length too short\")\n",
        "        print(t_data.shape)\n",
        "        continue\n",
        "      \n",
        "      max_v = t_labels.max()\n",
        "      \n",
        "      if max_v > 3:\n",
        "        #print(\"\\nlabels were not correct\" , max_v , \", skipped batch: \" , i)\n",
        "        continue\n",
        "      \n",
        "      #zero mean and normalize the data\n",
        "      mean = t_data.mean()\n",
        "      std  = t_data.std()\n",
        "      \n",
        "      t_data = (t_data - mean)/std\n",
        "      \n",
        "      # Load data into GPU\n",
        "      inputs = t_data.float().to(device)\n",
        "    \n",
        "      \n",
        "      targets = torch.LongTensor(t_labels.reshape((4,seq))[0:4,seq-1]).to(device)\n",
        "        \n",
        "      # Forward pass\n",
        "      #outputs = net(inputs).reshape([inputs.shape[0],4])\n",
        "      outputs = net(inputs).reshape([4,seq,4])[0:4,seq-1]\n",
        "      \n",
        "      \n",
        "      # Apply the loss\n",
        "      loss = cost_function(outputs, targets)\n",
        "\n",
        "      # Backward pass\n",
        "      loss.backward()\n",
        "    \n",
        "      # Update parameters\n",
        "      optimizer.step()\n",
        "    \n",
        "      # Reset the optimizer\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      samples+=4\n",
        "        \n",
        "      sequences+=4\n",
        "        \n",
        "      cumulative_loss += loss.item() # Note: the .item() is needed to extract scalars from tensors\n",
        "        \n",
        "      _, predicted = outputs.max(1)\n",
        "      \n",
        "      #print(\"outputs from rnn -> \", rnnOutputs.shape)\n",
        "        \n",
        "      #seq_results = outputs.view(4,seq,outputs.shape[1])[0:4 , seq-1] # (4,4)\n",
        "        \n",
        "      #print(\"last outputs ->\", seq_results, seq_results.shape)\n",
        "        \n",
        "      #_, predicted_s = seq_results.max(1)\n",
        "        \n",
        "      #print(\"max predicted value\",predicted_s)\n",
        "        \n",
        "      #seq_targets   = targets.view(4,seq)[0:4,seq-1]\n",
        "        \n",
        "      #print(\"seq_targets -> \", seq_targets)\n",
        "        \n",
        "      #cumulative_seq_accuracy += predicted_s.eq(seq_targets).sum().item()\n",
        "        \n",
        "      cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "      \n",
        "    print(\"Accuracy after subject evaluation -> \" , cumulative_accuracy/samples*100)\n",
        "    #print(\"Sequence Accuracy after subject evaluation -> \" , cumulative_seq_accuracy/sequences*100)\n",
        "      \n",
        "    print( int(100*skipped/(skipped+kept)) ,\"% of the data was ignored due to sequence cleaning\")\n",
        "  \n",
        "  \n",
        "  return cumulative_loss/samples, cumulative_accuracy/samples*100, cumulative_accuracy/samples*100# cumulative_seq_accuracy/sequences*100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WS_DYJC4ZdSl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_seq(data,seq,seq_len=200):\n",
        "  \n",
        "  if np.amax(seq) == 0:\n",
        "    #print(\"returned something\")\n",
        "    return [data[0:seq_len],seq[0:seq_len]]\n",
        "  \n",
        "  #size of sequence\n",
        "  seq_size = seq.size\n",
        "  \n",
        "  #get the first snippet\n",
        "  \n",
        "  first_good = False\n",
        "  last_good  = False\n",
        "  i=0\n",
        "  \n",
        "  while not last_good:\n",
        "  \n",
        "    while not first_good:\n",
        "      \n",
        "      snippet =       seq[seq_size-seq_len-i:seq_size-i]\n",
        "      data_snippet = data[seq_size-seq_len-i:seq_size-i]\n",
        "  \n",
        "      #first value\n",
        "      first_value = snippet[0]\n",
        "  \n",
        "      if first_value.item() == 0:\n",
        "        #print(\"first value is good\",first_value)\n",
        "        first_good = True\n",
        "      elif i == 0:\n",
        "        i = seq_size-seq_len\n",
        "        #print(\"moved snippet 1\")\n",
        "      else:\n",
        "        #print(\"couldn't make sequence\")\n",
        "        #print(snippet)\n",
        "        return\n",
        "  \n",
        "  \n",
        "    #last value\n",
        "    last_value = snippet[-1]\n",
        "  \n",
        "    #max value\n",
        "    max_value =  np.amax(snippet)\n",
        "    #print(\"max is -> \",max_value)\n",
        "    \n",
        "    if max_value == last_value:\n",
        "      #print(\"last value is good\",last_value)\n",
        "      last_good = True\n",
        "    elif i == 0:\n",
        "      first_good = False\n",
        "      i = seq_size-seq_len\n",
        "      #print(\"moved snippet 2\")\n",
        "    elif not last_good or not first_good:\n",
        "      #print(\"couldn't make sequence\")\n",
        "      #print(snippet)\n",
        "      return\n",
        "    \n",
        "  #print(snippet)\n",
        "  \n",
        "  return [data_snippet,snippet]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cAMMKKciyX1",
        "colab_type": "text"
      },
      "source": [
        "## Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvBsTe0vBWb2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_net(path):\n",
        "  with open(path, 'rb') as f:\n",
        "    result = pickle.load(f)\n",
        "  \n",
        "  return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1U9ZogaTimXY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main(net= None,\n",
        "         netname= 'net',\n",
        "         batch_size=128, \n",
        "         device='cuda:0', \n",
        "         learning_rate=1e-5, \n",
        "         weight_decay=0.000001, \n",
        "         momentum=0.9, \n",
        "         epochs=25,\n",
        "         start=0,\n",
        "         end =16):\n",
        "  \n",
        "  print(\"make data loaders\")\n",
        "  \n",
        "  train_loader = \"train\"\n",
        "  val_loader   = \"val\"\n",
        "  \n",
        "  print(\"network to device\")\n",
        "  if net == None:\n",
        "    net = eeg_CNN(netname).to(device)\n",
        "  else:\n",
        "    print(\"continue training on net\")\n",
        "    net.to(device)\n",
        "    net.name = netname\n",
        "  \n",
        "  print(\"optimizer\")\n",
        "  optimizer = get_optimizer(net , learning_rate, weight_decay, momentum)\n",
        "  \n",
        "  print(\"cost function\")\n",
        "  cost_function = get_cost_function()\n",
        "\n",
        "  #print('Before training:')\n",
        "  #train_loss, train_accuracy, seq_train_accuracy = test(net, train_loader, cost_function, device,start,end)\n",
        "  #val_loss, val_accuracy, seq_val_accuracy      = test(net, val_loader, cost_function, device,start,end)\n",
        "  \n",
        "  #print('\\t Training loss {:.5f}, Training accuracy {:.2f}, Sequence Training accuracy {:.2f}'.format(train_loss, train_accuracy, seq_rain_accuracy))\n",
        "  #print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}, Sequence Validation accuracy {:.2f}'.format(val_loss, val_accuracy, seq_val_accuracy))\n",
        "  #print('-----------------------------------------------------')\n",
        "  \n",
        "  lr = learning_rate\n",
        "  #loss = train_loss\n",
        "  loss = 0.30160\n",
        "  \n",
        "  for e in range(epochs):\n",
        "    train_loss, train_accuracy, seq_train_accuracy = train(net, train_loader, optimizer, cost_function,device,start,end)\n",
        "    \n",
        "    dloss = 1 - train_loss/loss\n",
        "    \n",
        "    loss = train_loss\n",
        "    \n",
        "    print(\"change in loss is:\" , dloss)\n",
        "    \n",
        "    lr = learning_rate + abs(math.cos(e/2)*learning_rate*10)\n",
        "    \n",
        "    print(\"new lr ->\", lr)\n",
        "      \n",
        "    optimizer = get_optimizer(net, lr, weight_decay, momentum)\n",
        "    \n",
        "    #val_loss, val_accuracy, seq_val_accuracy = test(net, val_loader, cost_function)\n",
        "    print('Epoch: {:d}'.format(e+1))\n",
        "    print('\\t Training loss {:.5f}, Training accuracy {:.2f}, Sequence Training accuracy {:.2f}'.format(train_loss, train_accuracy, seq_train_accuracy))\n",
        "    #print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}, Sequence Validation accuracy {:.2f}'.format(val_loss, val_accuracy, seq_val_accuracy))\n",
        "    print('-----------------------------------------------------')\n",
        "    \n",
        "    net.next_epoch(lr,train_accuracy,train_loss)\n",
        "    net.save()\n",
        "    \n",
        "    \n",
        "\n",
        "  #print('After training:')\n",
        "  #train_loss, train_accuracy, seq_train_accuracy = test(net, train_loader, cost_function, device,start,end)\n",
        "  #val_loss, val_accuracy, seq_val_accuracy      = test(net, val_loader, cost_function, device,start,end)\n",
        "  #test_loss, test_accuracy = test(net, test_loader, cost_function)\n",
        "\n",
        "  #print('\\t Training loss {:.5f}, Training accuracy {:.2f}, Sequence Training accuracy {:.2f}'.format(train_loss, train_accuracy, seq_rain_accuracy))\n",
        "  #print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy, seq_val_accuracy))\n",
        "  #print('\\t Test loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "  #print('-----------------------------------------------------')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poFRstlzluSP",
        "colab_type": "code",
        "outputId": "c2e9e9e0-1727-4f11-c997-b18889973616",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "#net = get_net('gdrive/My Drive/DeepLearning/Nets/11-layer-CNN-37.pickle')\n",
        "\n",
        "#try a new one\n",
        "#net = eeg_shallow_CNN('CNN-LSTM-1')\n",
        "net = get_net('gdrive/My Drive/DeepLearning/Nets/CNN-LSTM-16.pickle')\n",
        "#net.reset()\n",
        "\n",
        "#print(\"restarting on net from epoch \", net.training_epochs)\n",
        "#print(\"current accuracy -> \",net.accuracy_history[-1])\n",
        "\n",
        "main(net,\n",
        "         netname='CNN-LSTM-all',\n",
        "         batch_size=128, \n",
        "         device='cuda:0', \n",
        "         learning_rate=5e-4, \n",
        "         weight_decay=1e-9, \n",
        "         momentum=0.9, \n",
        "         epochs=5,\n",
        "         start=0,\n",
        "         end  =16+1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "make data loaders\n",
            "network to device\n",
            "continue training on net\n",
            "optimizer\n",
            "cost function\n",
            "\n",
            "Training on subject  0\n",
            "on train batch:  0 - 300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Utb8vIzYBtpo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E82xm2a5-TxF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}