{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN -simple",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "QXaFp2bbtQAN",
        "vww_wREStmg3",
        "i5LOmkkPu5ye",
        "vtP2QmbcvfkI"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXaFp2bbtQAN",
        "colab_type": "text"
      },
      "source": [
        "## A CNN to classify EEG data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoJhN7K7tNSh",
        "colab_type": "code",
        "outputId": "726d70e6-ea0f-4be2-990a-34654268339b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "!pip install pydrive\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import roc_auc_score, precision_score, recall_score, accuracy_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "##### START OF ADDITION OF MY CODE\n",
        "\n",
        "import numpy as np\n",
        "import os.path\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import io\n",
        "from googleapiclient.http import MediaIoBaseDownload\n",
        "\n",
        "from scipy.io import loadmat\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io as sio\n",
        "import sys\n",
        "\n",
        "## PyTorch \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "from torch import Tensor\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "\n",
        "import math #for calculus\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pydrive\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/e0/0e64788e5dd58ce2d6934549676243dc69d982f198524be9b99e9c2a4fd5/PyDrive-1.3.1.tar.gz (987kB)\n",
            "\u001b[K     |████████████████████████████████| 993kB 4.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.6/dist-packages (from pydrive) (1.6.7)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pydrive) (4.1.3)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.6/dist-packages (from pydrive) (3.13)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->pydrive) (3.0.0)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->pydrive) (0.11.3)\n",
            "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->pydrive) (1.12.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive) (0.4.5)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive) (4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive) (0.2.5)\n",
            "Building wheels for collected packages: pydrive\n",
            "  Building wheel for pydrive (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/fa/d2/9a/d3b6b506c2da98289e5d417215ce34b696db856643bad779f4\n",
            "Successfully built pydrive\n",
            "Installing collected packages: pydrive\n",
            "Successfully installed pydrive-1.3.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vww_wREStmg3",
        "colab_type": "text"
      },
      "source": [
        "## Download the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMwiZlpXtohQ",
        "colab_type": "code",
        "outputId": "5c79ca47-362d-467e-9820-7439a4249d04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "file_id = '1al08tF3j-Z2-fowjPPNz1SrFgLVqywXG' ##1.mat\n",
        "file_id = '1TqewoCjjRXZpEIxL_23ZQ8MP4L0RofZx'\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.GetContentFile('./train3.mat')\n",
        "\n",
        "mat = sio.loadmat('train3.mat', squeeze_me=True, struct_as_record=False)\n",
        "\n",
        "print(\"loaded dataset  ...\")\n",
        "\n",
        "o=mat['o']\n",
        "data=mat['o'].data\n",
        "labels=mat['o'].marker\n",
        "\n",
        "newlabels = np.zeros([data.shape[0],4])\n",
        "found = np.zeros([1,])\n",
        "\n",
        "#clean the shit out of it\n",
        "while np.amax(labels) > 3:\n",
        "  ind = np.argmax(labels)\n",
        "  found = np.append(found,np.array([ind]), axis=0)\n",
        "  labels[ind] = 0\n",
        "  data[ind,:] = np.zeros([22,])\n",
        "    \n",
        "\n",
        "print(\"cleaned data from breaks  ...\")\n",
        "\n",
        "labeled_data = (data,labels)\n",
        "  "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loaded dataset  ...\n",
            "cleaned data from breaks  ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MxBINl_L1d7",
        "colab_type": "code",
        "outputId": "eb5aea17-8bb4-4be8-c9b7-6f45525ed820",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(data.shape)\n",
        "print(labels.shape)\n",
        "\n",
        "#adding the label info directly to the data\n",
        "data = np.hstack((data,labels.reshape((labels.shape[0],1))))\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(667000, 22)\n",
            "(667000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5LOmkkPu5ye",
        "colab_type": "text"
      },
      "source": [
        "##Video Stream Creation\n",
        "\n",
        "The imported data is converted into a 3D image. The electrodes are reconstructed assuming the head is a 3 dimensional ellipse.\n",
        "The calculation can be seen at:\n",
        "\n",
        "https://docs.google.com/spreadsheets/d/1TetDbE-HkAQGnVVIFbPGI4ugbktgdic_TFtsFyWEp0w/edit?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YyiQIYlvF4D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## video maker and data extractor\n",
        "\n",
        "pos=np.zeros((22,3), dtype=int)\n",
        "\n",
        "pos[0]=-2,10,0\n",
        "pos[1]=2,10,0\n",
        "pos[3]=-4,7,3\n",
        "pos[4]=4,7,3\n",
        "pos[5]=-5,0,4\n",
        "pos[6]=5,0,4\n",
        "pos[7]=-4,-7,3\n",
        "pos[8]=4,-7,3\n",
        "pos[9]=-2,-10,0\n",
        "pos[10]=2,-10,0\n",
        "pos[11]=-7,0,-5\n",
        "pos[12]=7,0,5\n",
        "pos[13]=-6,6,0\n",
        "pos[14]=6,6,0\n",
        "pos[15]=-7,0,0\n",
        "pos[16]=7,0,0\n",
        "pos[17]=-6,-6,0\n",
        "pos[18]=6,-6,0\n",
        "pos[19]=0,7,4\n",
        "pos[20]=0,0,6\n",
        "pos[21]=0,-7,4\n",
        "\n",
        "def make_3d_point(x,y,z,r,theta,phi):\n",
        "  return [\n",
        "      x + int(r * math.cos(theta)*math.sin(phi)),\n",
        "      y + int(r * math.sin(theta)*math.sin(phi)),\n",
        "      z + int(r * math.cos(phi))\n",
        "         ]\n",
        "\n",
        "def make_3d_data(image3d_data):\n",
        "  #creating the matrix\n",
        "  matrix = np.zeros([12,16,22]) # z,x,y\n",
        "  \n",
        "  #looping through data\n",
        "  for point in image3d_data:\n",
        "    \n",
        "    # creating the indice by shifting the coordinate system from the center to left corner\n",
        "    newpoint = [int(point[0]) + 7,int(point[1]) + 10,int(point[2]) + 5]\n",
        "    \n",
        "    if newpoint[0] >= matrix.shape[1] or newpoint[1] >= matrix.shape[2] or newpoint[2] >= matrix.shape[0] or newpoint[0] < 0 or newpoint[1] < 0 or newpoint[2] < 0:\n",
        "      #print(\"did not add point\")\n",
        "      #print(newpoint)\n",
        "      #print(matrix.shape)\n",
        "      continue\n",
        "\n",
        "    \n",
        "    #assigning the value\n",
        "    if abs(matrix[newpoint[2],newpoint[0],newpoint[1]]) <= abs(point[3]):\n",
        "        matrix[newpoint[2],newpoint[0],newpoint[1]] = int(point[3])\n",
        "\n",
        "    \n",
        "    #creating variables for creating a virtual sphere around poi\n",
        "    rho = [2,3] # distance (in pixels) that we are interpolating from poi\n",
        "    #theta  angle in radians    [0,2pi]\n",
        "    #phi    angle in radians    [0, pi]\n",
        "    \n",
        "    #creating new interpolated points\n",
        "    for r in rho:\n",
        "      for theta in range(0,360,45):\n",
        "        for phi in range(0,180,45):\n",
        "          p = make_3d_point(newpoint[0],newpoint[1],newpoint[2],r,math.radians(theta),math.radians(phi))\n",
        "          \n",
        "          #checking if its in bounds\n",
        "          if p[0] < matrix.shape[1] and p[1] < matrix.shape[2] and p[2] < matrix.shape[0]:\n",
        "            if abs(matrix[p[2],p[0],p[1]]) <= abs(point[3]/r):\n",
        "                matrix[p[2],p[0],p[1]] = int(point[3]/r)\n",
        "                     \n",
        "  return matrix\n",
        "\n",
        "\n",
        "def get_data(batch_size, test_batch_size=256):\n",
        "  \n",
        "\n",
        "  # Create train and validation splits\n",
        "  num_samples = len(data)\n",
        "  training_samples = int(num_samples*0.5+1)\n",
        "  validation_samples = num_samples - training_samples\n",
        "\n",
        "  training_data, validation_data = torch.utils.data.random_split(data, [training_samples, validation_samples])\n",
        "\n",
        "  # Initialize dataloaders\n",
        "  train_loader = torch.utils.data.DataLoader(training_data, batch_size, shuffle=False)\n",
        "  val_loader = torch.utils.data.DataLoader(validation_data, batch_size, shuffle=False)\n",
        "  #test_loader = torch.utils.data.DataLoader(test_data, test_batch_size, shuffle=False)\n",
        "  \n",
        "  print(\"loading data\")\n",
        "  \n",
        "  return train_loader, val_loader #, test_loader\n",
        "\n",
        "\n",
        "\n",
        "def get_video_data(loader):\n",
        "  \n",
        "  #print(\"get videos:\")\n",
        "\n",
        "  batchsize = loader.shape[0]\n",
        "  \n",
        "  training_data   = np.empty([batchsize,22,4])\n",
        "  \n",
        "  train_label = np.empty((batchsize))\n",
        "  \n",
        " # print(\"start collecting training data\")\n",
        "  \n",
        "  for it,timestep_data in enumerate(loader):\n",
        "    train3d = np.hstack((pos,timestep_data[0:22].reshape([22,1])))\n",
        "    training_data[it] = train3d\n",
        "    train_label[it] = timestep_data[22] \n",
        " \n",
        "    \n",
        "  return (training_data, train_label)\n",
        "  \n",
        "  \n",
        "  \n",
        "def make_video(video_data):\n",
        "  return make_3d_data(video_data)\n",
        "  \n",
        "  \n",
        "  \n",
        "def get_training_data(train):\n",
        "\n",
        "  stream = np.zeros([train[0].shape[0],1,12,16,22])\n",
        "  \n",
        "  for i,image in enumerate(train[0]):\n",
        "    if i >= stream.shape[0]:\n",
        "      break\n",
        "    stream[i,0] = make_video(image)\n",
        "  \n",
        "  tensor = torch.from_numpy(stream)\n",
        "\n",
        "  min_v = np.amax(data)\n",
        "\n",
        "  max_v = np.amin(data)\n",
        "\n",
        "  limit = max(abs(min_v),abs(max_v))\n",
        "\n",
        "  tensor = tensor/limit\n",
        "  \n",
        "  return (tensor,train[1])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZmEYzOGfeBk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#get_training_data(get_data(254),254)\n",
        "#get_test_data(get_data(254),254)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtP2QmbcvfkI",
        "colab_type": "text"
      },
      "source": [
        "## CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bASv73CvfD5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class eeg_CNN(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(eeg_CNN, self).__init__()\n",
        "        self.T = 120\n",
        "        \n",
        "        self.conv1 = nn.Conv3d(1,5,2, stride=1,padding=(1,1,1),dilation=1)\n",
        "        self.batchnorm1 = nn.BatchNorm3d(5)\n",
        "        self.conv2 = nn.Conv3d(5,10,(2,3,2), stride=1,padding=(1,1,1),dilation=1)\n",
        "        self.batchnorm2 = nn.BatchNorm3d(10)\n",
        "        self.conv3 = nn.Conv3d(10,5,3, stride=1,dilation=1)\n",
        "        self.batchnorm3 = nn.BatchNorm3d(5)\n",
        "        self.conv4 = nn.Conv3d(5,5,(3,3,2), stride=1, dilation =1)\n",
        "        self.batchnorm4 = nn.BatchNorm3d(5)\n",
        "        self.conv5 = nn.Conv3d(5,5,(1,2,1), stride=1, dilation =1)\n",
        "        self.batchnorm5 = nn.BatchNorm3d(5)\n",
        "        self.pooling1 = nn.MaxPool3d((2,2,2), stride=1, dilation=1)\n",
        "        \n",
        "        #layer after max pool\n",
        "        self.conv6 = nn.Conv3d(5,5,3, stride=1,padding=(1,1,1),dilation=1)\n",
        "        self.batchnorm6 = nn.BatchNorm3d(5)\n",
        "        self.conv7 = nn.Conv3d(5,5,(3,3,2), stride=1,dilation=1)\n",
        "        self.batchnorm7 = nn.BatchNorm3d(5)\n",
        "        self.conv8 = nn.Conv3d(5,5,(1,2,1), stride=1,dilation=1)\n",
        "        self.batchnorm8 = nn.BatchNorm3d(5)\n",
        "        self.pooling2 = nn.MaxPool3d((3,3,2), stride=1, dilation=1)\n",
        "        \n",
        "        #Layer after next max pool \n",
        "        self.conv9 = nn.Conv3d(5,5,3, stride=1,padding=(1,1,1),dilation=1)\n",
        "        self.batchnorm9 = nn.BatchNorm3d(5)\n",
        "        self.conv10 = nn.Conv3d(5,5,(3,3,2), stride=1,dilation=1)\n",
        "        self.batchnorm10 = nn.BatchNorm3d(5)\n",
        "        self.conv11 = nn.Conv3d(5,1,(1,2,1), stride=1,dilation=1)\n",
        "        self.batchnorm11 = nn.BatchNorm3d(5)\n",
        "        self.pooling3 = nn.MaxPool3d((3,3,2), stride=1, dilation=1)\n",
        "        \n",
        "        self.fc1 = torch.nn.Linear(in_features=1 * 1 * 16, out_features=4)\n",
        "      \n",
        "\n",
        "        \n",
        "    def forward(self, x):\n",
        "\n",
        "        # first set of CNNs and then a max pool\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.batchnorm1(x)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.batchnorm2(x)\n",
        "        x = F.relu(self.conv3(x)) \n",
        "        x = self.batchnorm3(x)\n",
        "        x = F.relu(self.conv4(x))\n",
        "        x = self.batchnorm4(x)\n",
        "        x = F.relu(self.conv5(x))\n",
        "        x = self.batchnorm5(x)\n",
        "        x = self.pooling1(x)\n",
        "        \n",
        "        # second set\n",
        "        \n",
        "        x = F.relu(self.conv6(x))\n",
        "        x = self.batchnorm6(x)\n",
        "        x = F.relu(self.conv7(x))\n",
        "        x = self.batchnorm7(x)\n",
        "        x = F.relu(self.conv8(x))\n",
        "        x = self.batchnorm9(x)\n",
        "        x = self.pooling2(x)\n",
        "        \n",
        "        # set 3  \n",
        "        \n",
        "        x = F.relu(self.conv9(x))\n",
        "        x = self.batchnorm10(x)\n",
        "        x = F.relu(self.conv10(x))\n",
        "        x = self.batchnorm11(x)\n",
        "        x = F.relu(self.conv11(x))\n",
        "        x = self.pooling3(x)\n",
        "        \n",
        "        batch_size, timesteps, C, H, W = x.size()\n",
        "        \n",
        "        x = x.view(batch_size * timesteps, C, H, W)\n",
        "        \n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        \n",
        "        return F.softmax(x,dim=3)\n",
        "  \n",
        "def get_cost_function():\n",
        "    \n",
        "  cost_function = torch.nn.CrossEntropyLoss()\n",
        "  return cost_function\n",
        "    \n",
        "def get_optimizer(net, lr, wd, momentum):\n",
        "    \n",
        "  optimizer = torch.optim.SGD(net.parameters(), lr=lr, weight_decay=wd, momentum=momentum)\n",
        "  return optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CILv1uophoHm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(net, data_loader, cost_function, device='cuda:0'):\n",
        "  samples = 0.\n",
        "  cumulative_loss = 0.\n",
        "  cumulative_accuracy = 0.\n",
        "\n",
        "  net.eval() # Strictly needed if network contains layers which has different behaviours between train and test\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    \n",
        "    for batch_idx, inputs in enumerate(data_loader):\n",
        "\n",
        "      if batch_idx % 50 == 0:\n",
        "        print(\"on test batch: \" , batch_idx, \"-\", batch_idx+50)\n",
        "      \n",
        "      result_data = get_video_data(inputs)\n",
        "      result = get_training_data(result_data)\n",
        "\n",
        "      \n",
        "      t_data   = result[0]\n",
        "      t_labels = result[1]\n",
        "      \n",
        "      # Load data into GPU\n",
        "      inputs = t_data.float().to(device)\n",
        "    \n",
        "      targets = torch.LongTensor(t_labels).to(device)\n",
        "        \n",
        "      # Forward pass\n",
        "      outputs = net(inputs).reshape([inputs.shape[0],4])\n",
        "\n",
        "      #print(outputs) \n",
        "      #print(targets)\n",
        "\n",
        "      # Apply the loss\n",
        "      loss = cost_function(outputs, targets)\n",
        "      #print(\"loss\",loss)\n",
        "\n",
        "      # Better print something\n",
        "      samples+=inputs.shape[0]\n",
        "      cumulative_loss += loss.item() # Note: the .item() is needed to extract scalars from tensors\n",
        "      _, predicted = outputs.max(1)\n",
        "      cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "      \n",
        "\n",
        "  return cumulative_loss/samples, cumulative_accuracy/samples*100\n",
        "\n",
        "\n",
        "def train(net,data_loader,optimizer,cost_function, device='cuda:0'):\n",
        "  samples = 0.\n",
        "  cumulative_loss = 0.\n",
        "  cumulative_accuracy = 0.\n",
        "\n",
        "  \n",
        "  net.train() # Strictly needed if network contains layers which has different behaviours between train and test\n",
        "  for batch_idx, inputs in enumerate(data_loader):\n",
        "\n",
        "    if batch_idx % 50 == 0:\n",
        "        print(\"on train batch: \" , batch_idx, \"-\", batch_idx+50)\n",
        "      \n",
        "    result_data = get_video_data(inputs)\n",
        "    result = get_training_data(result_data)\n",
        "\n",
        "    t_data   = result[0]\n",
        "    t_labels = result[1]\n",
        "      \n",
        "    # Load data into GPU\n",
        "    inputs = t_data.float().to(device)\n",
        "    \n",
        "    targets = torch.LongTensor(t_labels).to(device)\n",
        "        \n",
        "    # Forward pass\n",
        "    outputs = net(inputs).reshape([inputs.shape[0],4])\n",
        "\n",
        "    # Apply the loss\n",
        "    loss = cost_function(outputs, targets)\n",
        "\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    \n",
        "    # Update parameters\n",
        "    optimizer.step()\n",
        "    \n",
        "    # Reset the optimizer\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Better print something, no?\n",
        "    samples+=inputs.shape[0]\n",
        "    cumulative_loss += loss.item()\n",
        "    _, predicted = outputs.max(1)\n",
        "    cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "  return cumulative_loss/samples, cumulative_accuracy/samples*100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1U9ZogaTimXY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main(batch_size=128, \n",
        "         device='cuda:0', \n",
        "         learning_rate=0.01, \n",
        "         weight_decay=0.000001, \n",
        "         momentum=0.9, \n",
        "         epochs=25):\n",
        "  \n",
        "  print(\"make data loaders\")\n",
        "  \n",
        "  train_loader, val_loader = get_data(batch_size)\n",
        "  \n",
        "  print(\"network to device\")\n",
        "  net = eeg_CNN().to(device)\n",
        "  \n",
        "  print(\"optimizer\")\n",
        "  optimizer = get_optimizer(net, learning_rate, weight_decay, momentum)\n",
        "  \n",
        "  print(\"cost function\")\n",
        "  cost_function = get_cost_function()\n",
        "\n",
        "  #print('Before training:')\n",
        "  #train_loss, train_accuracy = test(net, train_loader, cost_function)\n",
        "  #val_loss, val_accuracy = test(net, val_loader, cost_function)\n",
        "  \n",
        "  #print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "  #print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "  #print('-----------------------------------------------------')\n",
        "  \n",
        "  for e in range(epochs):\n",
        "    train_loss, train_accuracy = train(net, train_loader, optimizer, cost_function)\n",
        "    val_loss, val_accuracy = test(net, val_loader, cost_function)\n",
        "    print('Epoch: {:d}'.format(e+1))\n",
        "    print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "    print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "    print('-----------------------------------------------------')\n",
        "\n",
        "  print('After training:')\n",
        "  train_loss, train_accuracy = test(net, train_loader, cost_function)\n",
        "  val_loss, val_accuracy = test(net, val_loader, cost_function)\n",
        "  #test_loss, test_accuracy = test(net, test_loader, cost_function)\n",
        "\n",
        "  print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "  print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "  #print('\\t Test loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "  print('-----------------------------------------------------')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cAMMKKciyX1",
        "colab_type": "text"
      },
      "source": [
        "## Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poFRstlzluSP",
        "colab_type": "code",
        "outputId": "b32031ce-e9ac-477a-c73e-cb672a6cef58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6138
        }
      },
      "source": [
        "main()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "make data loaders\n",
            "loading data\n",
            "network to device\n",
            "optimizer\n",
            "cost function\n",
            "on train batch:  0 - 50\n",
            "on train batch:  50 - 100\n",
            "on train batch:  100 - 150\n",
            "on train batch:  150 - 200\n",
            "on train batch:  200 - 250\n",
            "on train batch:  250 - 300\n",
            "on train batch:  300 - 350\n",
            "on train batch:  350 - 400\n",
            "on train batch:  400 - 450\n",
            "on train batch:  450 - 500\n",
            "on train batch:  500 - 550\n",
            "on train batch:  550 - 600\n",
            "on train batch:  600 - 650\n",
            "on train batch:  650 - 700\n",
            "on train batch:  700 - 750\n",
            "on train batch:  750 - 800\n",
            "on train batch:  800 - 850\n",
            "on train batch:  850 - 900\n",
            "on train batch:  900 - 950\n",
            "on train batch:  950 - 1000\n",
            "on train batch:  1000 - 1050\n",
            "on train batch:  1050 - 1100\n",
            "on train batch:  1100 - 1150\n",
            "on train batch:  1150 - 1200\n",
            "on train batch:  1200 - 1250\n",
            "on train batch:  1250 - 1300\n",
            "on train batch:  1300 - 1350\n",
            "on train batch:  1350 - 1400\n",
            "on train batch:  1400 - 1450\n",
            "on train batch:  1450 - 1500\n",
            "on train batch:  1500 - 1550\n",
            "on train batch:  1550 - 1600\n",
            "on train batch:  1600 - 1650\n",
            "on train batch:  1650 - 1700\n",
            "on train batch:  1700 - 1750\n",
            "on train batch:  1750 - 1800\n",
            "on train batch:  1800 - 1850\n",
            "on train batch:  1850 - 1900\n",
            "on train batch:  1900 - 1950\n",
            "on train batch:  1950 - 2000\n",
            "on train batch:  2000 - 2050\n",
            "on train batch:  2050 - 2100\n",
            "on train batch:  2100 - 2150\n",
            "on train batch:  2150 - 2200\n",
            "on train batch:  2200 - 2250\n",
            "on train batch:  2250 - 2300\n",
            "on train batch:  2300 - 2350\n",
            "on train batch:  2350 - 2400\n",
            "on train batch:  2400 - 2450\n",
            "on train batch:  2450 - 2500\n",
            "on train batch:  2500 - 2550\n",
            "on train batch:  2550 - 2600\n",
            "on train batch:  2600 - 2650\n",
            "on test batch:  0 - 50\n",
            "on test batch:  50 - 100\n",
            "on test batch:  100 - 150\n",
            "on test batch:  150 - 200\n",
            "on test batch:  200 - 250\n",
            "on test batch:  250 - 300\n",
            "on test batch:  300 - 350\n",
            "on test batch:  350 - 400\n",
            "on test batch:  400 - 450\n",
            "on test batch:  450 - 500\n",
            "on test batch:  500 - 550\n",
            "on test batch:  550 - 600\n",
            "on test batch:  600 - 650\n",
            "on test batch:  650 - 700\n",
            "on test batch:  700 - 750\n",
            "on test batch:  750 - 800\n",
            "on test batch:  800 - 850\n",
            "on test batch:  850 - 900\n",
            "on test batch:  900 - 950\n",
            "on test batch:  950 - 1000\n",
            "on test batch:  1000 - 1050\n",
            "on test batch:  1050 - 1100\n",
            "on test batch:  1100 - 1150\n",
            "on test batch:  1150 - 1200\n",
            "on test batch:  1200 - 1250\n",
            "on test batch:  1250 - 1300\n",
            "on test batch:  1300 - 1350\n",
            "on test batch:  1350 - 1400\n",
            "on test batch:  1400 - 1450\n",
            "on test batch:  1450 - 1500\n",
            "on test batch:  1500 - 1550\n",
            "on test batch:  1550 - 1600\n",
            "on test batch:  1600 - 1650\n",
            "on test batch:  1650 - 1700\n",
            "on test batch:  1700 - 1750\n",
            "on test batch:  1750 - 1800\n",
            "on test batch:  1800 - 1850\n",
            "on test batch:  1850 - 1900\n",
            "on test batch:  1900 - 1950\n",
            "on test batch:  1950 - 2000\n",
            "on test batch:  2000 - 2050\n",
            "on test batch:  2050 - 2100\n",
            "on test batch:  2100 - 2150\n",
            "on test batch:  2150 - 2200\n",
            "on test batch:  2200 - 2250\n",
            "on test batch:  2250 - 2300\n",
            "on test batch:  2300 - 2350\n",
            "on test batch:  2350 - 2400\n",
            "on test batch:  2400 - 2450\n",
            "on test batch:  2450 - 2500\n",
            "on test batch:  2500 - 2550\n",
            "on test batch:  2550 - 2600\n",
            "on test batch:  2600 - 2650\n",
            "Epoch: 1\n",
            "\t Training loss 0.00810, Training accuracy 70.88\n",
            "\t Validation loss 0.00809, Validation accuracy 70.78\n",
            "-----------------------------------------------------\n",
            "on train batch:  0 - 50\n",
            "on train batch:  50 - 100\n",
            "on train batch:  100 - 150\n",
            "on train batch:  150 - 200\n",
            "on train batch:  200 - 250\n",
            "on train batch:  250 - 300\n",
            "on train batch:  300 - 350\n",
            "on train batch:  350 - 400\n",
            "on train batch:  400 - 450\n",
            "on train batch:  450 - 500\n",
            "on train batch:  500 - 550\n",
            "on train batch:  550 - 600\n",
            "on train batch:  600 - 650\n",
            "on train batch:  650 - 700\n",
            "on train batch:  700 - 750\n",
            "on train batch:  750 - 800\n",
            "on train batch:  800 - 850\n",
            "on train batch:  850 - 900\n",
            "on train batch:  900 - 950\n",
            "on train batch:  950 - 1000\n",
            "on train batch:  1000 - 1050\n",
            "on train batch:  1050 - 1100\n",
            "on train batch:  1100 - 1150\n",
            "on train batch:  1150 - 1200\n",
            "on train batch:  1200 - 1250\n",
            "on train batch:  1250 - 1300\n",
            "on train batch:  1300 - 1350\n",
            "on train batch:  1350 - 1400\n",
            "on train batch:  1400 - 1450\n",
            "on train batch:  1450 - 1500\n",
            "on train batch:  1500 - 1550\n",
            "on train batch:  1550 - 1600\n",
            "on train batch:  1600 - 1650\n",
            "on train batch:  1650 - 1700\n",
            "on train batch:  1700 - 1750\n",
            "on train batch:  1750 - 1800\n",
            "on train batch:  1800 - 1850\n",
            "on train batch:  1850 - 1900\n",
            "on train batch:  1900 - 1950\n",
            "on train batch:  1950 - 2000\n",
            "on train batch:  2000 - 2050\n",
            "on train batch:  2050 - 2100\n",
            "on train batch:  2100 - 2150\n",
            "on train batch:  2150 - 2200\n",
            "on train batch:  2200 - 2250\n",
            "on train batch:  2250 - 2300\n",
            "on train batch:  2300 - 2350\n",
            "on train batch:  2350 - 2400\n",
            "on train batch:  2400 - 2450\n",
            "on train batch:  2450 - 2500\n",
            "on train batch:  2500 - 2550\n",
            "on train batch:  2550 - 2600\n",
            "on train batch:  2600 - 2650\n",
            "on test batch:  0 - 50\n",
            "on test batch:  50 - 100\n",
            "on test batch:  100 - 150\n",
            "on test batch:  150 - 200\n",
            "on test batch:  200 - 250\n",
            "on test batch:  250 - 300\n",
            "on test batch:  300 - 350\n",
            "on test batch:  350 - 400\n",
            "on test batch:  400 - 450\n",
            "on test batch:  450 - 500\n",
            "on test batch:  500 - 550\n",
            "on test batch:  550 - 600\n",
            "on test batch:  600 - 650\n",
            "on test batch:  650 - 700\n",
            "on test batch:  700 - 750\n",
            "on test batch:  750 - 800\n",
            "on test batch:  800 - 850\n",
            "on test batch:  850 - 900\n",
            "on test batch:  900 - 950\n",
            "on test batch:  950 - 1000\n",
            "on test batch:  1000 - 1050\n",
            "on test batch:  1050 - 1100\n",
            "on test batch:  1100 - 1150\n",
            "on test batch:  1150 - 1200\n",
            "on test batch:  1200 - 1250\n",
            "on test batch:  1250 - 1300\n",
            "on test batch:  1300 - 1350\n",
            "on test batch:  1350 - 1400\n",
            "on test batch:  1400 - 1450\n",
            "on test batch:  1450 - 1500\n",
            "on test batch:  1500 - 1550\n",
            "on test batch:  1550 - 1600\n",
            "on test batch:  1600 - 1650\n",
            "on test batch:  1650 - 1700\n",
            "on test batch:  1700 - 1750\n",
            "on test batch:  1750 - 1800\n",
            "on test batch:  1800 - 1850\n",
            "on test batch:  1850 - 1900\n",
            "on test batch:  1900 - 1950\n",
            "on test batch:  1950 - 2000\n",
            "on test batch:  2000 - 2050\n",
            "on test batch:  2050 - 2100\n",
            "on test batch:  2100 - 2150\n",
            "on test batch:  2150 - 2200\n",
            "on test batch:  2200 - 2250\n",
            "on test batch:  2250 - 2300\n",
            "on test batch:  2300 - 2350\n",
            "on test batch:  2350 - 2400\n",
            "on test batch:  2400 - 2450\n",
            "on test batch:  2450 - 2500\n",
            "on test batch:  2500 - 2550\n",
            "on test batch:  2550 - 2600\n",
            "on test batch:  2600 - 2650\n",
            "Epoch: 2\n",
            "\t Training loss 0.00808, Training accuracy 70.90\n",
            "\t Validation loss 0.00809, Validation accuracy 70.78\n",
            "-----------------------------------------------------\n",
            "on train batch:  0 - 50\n",
            "on train batch:  50 - 100\n",
            "on train batch:  100 - 150\n",
            "on train batch:  150 - 200\n",
            "on train batch:  200 - 250\n",
            "on train batch:  250 - 300\n",
            "on train batch:  300 - 350\n",
            "on train batch:  350 - 400\n",
            "on train batch:  400 - 450\n",
            "on train batch:  450 - 500\n",
            "on train batch:  500 - 550\n",
            "on train batch:  550 - 600\n",
            "on train batch:  600 - 650\n",
            "on train batch:  650 - 700\n",
            "on train batch:  700 - 750\n",
            "on train batch:  750 - 800\n",
            "on train batch:  800 - 850\n",
            "on train batch:  850 - 900\n",
            "on train batch:  900 - 950\n",
            "on train batch:  950 - 1000\n",
            "on train batch:  1000 - 1050\n",
            "on train batch:  1050 - 1100\n",
            "on train batch:  1100 - 1150\n",
            "on train batch:  1150 - 1200\n",
            "on train batch:  1200 - 1250\n",
            "on train batch:  1250 - 1300\n",
            "on train batch:  1300 - 1350\n",
            "on train batch:  1350 - 1400\n",
            "on train batch:  1400 - 1450\n",
            "on train batch:  1450 - 1500\n",
            "on train batch:  1500 - 1550\n",
            "on train batch:  1550 - 1600\n",
            "on train batch:  1600 - 1650\n",
            "on train batch:  1650 - 1700\n",
            "on train batch:  1700 - 1750\n",
            "on train batch:  1750 - 1800\n",
            "on train batch:  1800 - 1850\n",
            "on train batch:  1850 - 1900\n",
            "on train batch:  1900 - 1950\n",
            "on train batch:  1950 - 2000\n",
            "on train batch:  2000 - 2050\n",
            "on train batch:  2050 - 2100\n",
            "on train batch:  2100 - 2150\n",
            "on train batch:  2150 - 2200\n",
            "on train batch:  2200 - 2250\n",
            "on train batch:  2250 - 2300\n",
            "on train batch:  2300 - 2350\n",
            "on train batch:  2350 - 2400\n",
            "on train batch:  2400 - 2450\n",
            "on train batch:  2450 - 2500\n",
            "on train batch:  2500 - 2550\n",
            "on train batch:  2550 - 2600\n",
            "on train batch:  2600 - 2650\n",
            "on test batch:  0 - 50\n",
            "on test batch:  50 - 100\n",
            "on test batch:  100 - 150\n",
            "on test batch:  150 - 200\n",
            "on test batch:  200 - 250\n",
            "on test batch:  250 - 300\n",
            "on test batch:  300 - 350\n",
            "on test batch:  350 - 400\n",
            "on test batch:  400 - 450\n",
            "on test batch:  450 - 500\n",
            "on test batch:  500 - 550\n",
            "on test batch:  550 - 600\n",
            "on test batch:  600 - 650\n",
            "on test batch:  650 - 700\n",
            "on test batch:  700 - 750\n",
            "on test batch:  750 - 800\n",
            "on test batch:  800 - 850\n",
            "on test batch:  850 - 900\n",
            "on test batch:  900 - 950\n",
            "on test batch:  950 - 1000\n",
            "on test batch:  1000 - 1050\n",
            "on test batch:  1050 - 1100\n",
            "on test batch:  1100 - 1150\n",
            "on test batch:  1150 - 1200\n",
            "on test batch:  1200 - 1250\n",
            "on test batch:  1250 - 1300\n",
            "on test batch:  1300 - 1350\n",
            "on test batch:  1350 - 1400\n",
            "on test batch:  1400 - 1450\n",
            "on test batch:  1450 - 1500\n",
            "on test batch:  1500 - 1550\n",
            "on test batch:  1550 - 1600\n",
            "on test batch:  1600 - 1650\n",
            "on test batch:  1650 - 1700\n",
            "on test batch:  1700 - 1750\n",
            "on test batch:  1750 - 1800\n",
            "on test batch:  1800 - 1850\n",
            "on test batch:  1850 - 1900\n",
            "on test batch:  1900 - 1950\n",
            "on test batch:  1950 - 2000\n",
            "on test batch:  2000 - 2050\n",
            "on test batch:  2050 - 2100\n",
            "on test batch:  2100 - 2150\n",
            "on test batch:  2150 - 2200\n",
            "on test batch:  2200 - 2250\n",
            "on test batch:  2250 - 2300\n",
            "on test batch:  2300 - 2350\n",
            "on test batch:  2350 - 2400\n",
            "on test batch:  2400 - 2450\n",
            "on test batch:  2450 - 2500\n",
            "on test batch:  2500 - 2550\n",
            "on test batch:  2550 - 2600\n",
            "on test batch:  2600 - 2650\n",
            "Epoch: 3\n",
            "\t Training loss 0.00808, Training accuracy 70.90\n",
            "\t Validation loss 0.00809, Validation accuracy 70.78\n",
            "-----------------------------------------------------\n",
            "on train batch:  0 - 50\n",
            "on train batch:  50 - 100\n",
            "on train batch:  100 - 150\n",
            "on train batch:  150 - 200\n",
            "on train batch:  200 - 250\n",
            "on train batch:  250 - 300\n",
            "on train batch:  300 - 350\n",
            "on train batch:  350 - 400\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-14-0bf54aaf873d>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(batch_size, device, learning_rate, weight_decay, momentum, epochs)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch: {:d}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-ca6c8ac76766>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, data_loader, optimizer, cost_function, device)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mresult_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_video_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_training_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mt_data\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-706a4a44db05>\u001b[0m in \u001b[0;36mget_training_data\u001b[0;34m(train)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m       \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m     \u001b[0mstream\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m   \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-706a4a44db05>\u001b[0m in \u001b[0;36mmake_video\u001b[0;34m(video_data)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mmake_3d_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-706a4a44db05>\u001b[0m in \u001b[0;36mmake_3d_data\u001b[0;34m(image3d_data)\u001b[0m\n\u001b[1;32m     62\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mtheta\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m360\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m45\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mphi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m180\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m45\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m           \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_3d_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnewpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnewpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mradians\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mradians\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m           \u001b[0;31m#checking if its in bounds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-706a4a44db05>\u001b[0m in \u001b[0;36mmake_3d_point\u001b[0;34m(x, y, z, r, theta, phi)\u001b[0m\n\u001b[1;32m     27\u001b[0m   return [\n\u001b[1;32m     28\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m       \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m       \u001b[0mz\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m          ]\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_FTuCxxlxHz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E82xm2a5-TxF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}