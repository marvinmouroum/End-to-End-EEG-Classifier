{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN -simple",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "QXaFp2bbtQAN"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXaFp2bbtQAN",
        "colab_type": "text"
      },
      "source": [
        "## A CNN to classify EEG data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoJhN7K7tNSh",
        "colab_type": "code",
        "outputId": "82c1c220-9b2d-4364-db0f-3d398f9f102b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "!pip install pydrive\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import roc_auc_score, precision_score, recall_score, accuracy_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "##### START OF ADDITION OF MY CODE\n",
        "\n",
        "import numpy as np\n",
        "import os.path\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import io\n",
        "from googleapiclient.http import MediaIoBaseDownload\n",
        "\n",
        "from scipy.io import loadmat\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io as sio\n",
        "import sys\n",
        "import pickle\n",
        "\n",
        "## PyTorch \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "from torch import Tensor\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "\n",
        "import math #for calculus\n",
        "\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pydrive in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.6/dist-packages (from pydrive) (3.13)\n",
            "Requirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.6/dist-packages (from pydrive) (1.6.7)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pydrive) (4.1.3)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->pydrive) (0.11.3)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->pydrive) (3.0.0)\n",
            "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->pydrive) (1.12.0)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive) (4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive) (0.2.5)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive) (0.4.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vww_wREStmg3",
        "colab_type": "text"
      },
      "source": [
        "## Download the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMwiZlpXtohQ",
        "colab_type": "code",
        "outputId": "5c79ca47-362d-467e-9820-7439a4249d04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "file_id = '1al08tF3j-Z2-fowjPPNz1SrFgLVqywXG' ##1.mat\n",
        "file_id = '1TqewoCjjRXZpEIxL_23ZQ8MP4L0RofZx'\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.GetContentFile('./train3.mat')\n",
        "\n",
        "mat = sio.loadmat('train3.mat', squeeze_me=True, struct_as_record=False)\n",
        "\n",
        "print(\"loaded dataset  ...\")\n",
        "\n",
        "o=mat['o']\n",
        "data=mat['o'].data\n",
        "labels=mat['o'].marker\n",
        "\n",
        "newlabels = np.zeros([data.shape[0],4])\n",
        "found = np.zeros([1,])\n",
        "\n",
        "#clean the shit out of it\n",
        "while np.amax(labels) > 3:\n",
        "  ind = np.argmax(labels)\n",
        "  found = np.append(found,np.array([ind]), axis=0)\n",
        "  labels[ind] = 0\n",
        "  data[ind,:] = np.zeros([22,])\n",
        "    \n",
        "\n",
        "print(\"cleaned data from breaks  ...\")\n",
        "\n",
        "labeled_data = (data,labels)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loaded dataset  ...\n",
            "cleaned data from breaks  ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MxBINl_L1d7",
        "colab_type": "code",
        "outputId": "eb5aea17-8bb4-4be8-c9b7-6f45525ed820",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(data.shape)\n",
        "print(labels.shape)\n",
        "\n",
        "#adding the label info directly to the data\n",
        "data = np.hstack((data,labels.reshape((labels.shape[0],1))))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(667000, 22)\n",
            "(667000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5LOmkkPu5ye",
        "colab_type": "text"
      },
      "source": [
        "##Video Stream Creation\n",
        "\n",
        "The imported data is converted into a 3D image. The electrodes are reconstructed assuming the head is a 3 dimensional ellipse.\n",
        "The calculation can be seen at:\n",
        "\n",
        "https://docs.google.com/spreadsheets/d/1TetDbE-HkAQGnVVIFbPGI4ugbktgdic_TFtsFyWEp0w/edit?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YyiQIYlvF4D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## video maker and data extractor\n",
        "\n",
        "pos=np.zeros((22,3), dtype=int)\n",
        "\n",
        "pos[0]=-2,10,0\n",
        "pos[1]=2,10,0\n",
        "pos[3]=-4,7,3\n",
        "pos[4]=4,7,3\n",
        "pos[5]=-5,0,4\n",
        "pos[6]=5,0,4\n",
        "pos[7]=-4,-7,3\n",
        "pos[8]=4,-7,3\n",
        "pos[9]=-2,-10,0\n",
        "pos[10]=2,-10,0\n",
        "pos[11]=-7,0,-5\n",
        "pos[12]=7,0,5\n",
        "pos[13]=-6,6,0\n",
        "pos[14]=6,6,0\n",
        "pos[15]=-7,0,0\n",
        "pos[16]=7,0,0\n",
        "pos[17]=-6,-6,0\n",
        "pos[18]=6,-6,0\n",
        "pos[19]=0,7,4\n",
        "pos[20]=0,0,6\n",
        "pos[21]=0,-7,4\n",
        "\n",
        "def make_3d_point(x,y,z,r,theta,phi):\n",
        "  return [\n",
        "      x + int(r * math.cos(theta)*math.sin(phi)),\n",
        "      y + int(r * math.sin(theta)*math.sin(phi)),\n",
        "      z + int(r * math.cos(phi))\n",
        "         ]\n",
        "\n",
        "def make_3d_data(image3d_data):\n",
        "  #creating the matrix\n",
        "  matrix = np.zeros([12,16,22]) # z,x,y\n",
        "  \n",
        "  #looping through data\n",
        "  for point in image3d_data:\n",
        "    \n",
        "    # creating the indice by shifting the coordinate system from the center to left corner\n",
        "    newpoint = [int(point[0]) + 7,int(point[1]) + 10,int(point[2]) + 5]\n",
        "    \n",
        "    if newpoint[0] >= matrix.shape[1] or newpoint[1] >= matrix.shape[2] or newpoint[2] >= matrix.shape[0] or newpoint[0] < 0 or newpoint[1] < 0 or newpoint[2] < 0:\n",
        "      #print(\"did not add point\")\n",
        "      #print(newpoint)\n",
        "      #print(matrix.shape)\n",
        "      continue\n",
        "\n",
        "    \n",
        "    #assigning the value\n",
        "    if abs(matrix[newpoint[2],newpoint[0],newpoint[1]]) <= abs(point[3]):\n",
        "        matrix[newpoint[2],newpoint[0],newpoint[1]] = int(point[3])\n",
        "\n",
        "    \n",
        "    #creating variables for creating a virtual sphere around poi\n",
        "    rho = [2,3] # distance (in pixels) that we are interpolating from poi\n",
        "    #theta  angle in radians    [0,2pi]\n",
        "    #phi    angle in radians    [0, pi]\n",
        "    \n",
        "    #creating new interpolated points\n",
        "    for r in rho:\n",
        "      for theta in range(0,360,45):\n",
        "        for phi in range(0,180,45):\n",
        "          p = make_3d_point(newpoint[0],newpoint[1],newpoint[2],r,math.radians(theta),math.radians(phi))\n",
        "          \n",
        "          #checking if its in bounds\n",
        "          if p[0] < matrix.shape[1] and p[1] < matrix.shape[2] and p[2] < matrix.shape[0]:\n",
        "            if abs(matrix[p[2],p[0],p[1]]) <= abs(point[3]/r):\n",
        "                matrix[p[2],p[0],p[1]] = int(point[3]/r)\n",
        "                     \n",
        "  return matrix\n",
        "\n",
        "\n",
        "def get_data(batch_size, test_batch_size=256):\n",
        "  \n",
        "\n",
        "  # Create train and validation splits\n",
        "  num_samples = len(data)\n",
        "  training_samples = int(num_samples*0.5+1)\n",
        "  validation_samples = num_samples - training_samples\n",
        "\n",
        "  training_data, validation_data = torch.utils.data.random_split(data, [training_samples, validation_samples])\n",
        "\n",
        "  # Initialize dataloaders\n",
        "  train_loader = torch.utils.data.DataLoader(training_data, batch_size, shuffle=False)\n",
        "  val_loader = torch.utils.data.DataLoader(validation_data, batch_size, shuffle=False)\n",
        "  #test_loader = torch.utils.data.DataLoader(test_data, test_batch_size, shuffle=False)\n",
        "  \n",
        "  print(\"loading data\")\n",
        "  \n",
        "  return train_loader, val_loader #, test_loader\n",
        "\n",
        "\n",
        "\n",
        "def get_video_data(loader):\n",
        "  \n",
        "  #print(\"get videos:\")\n",
        "\n",
        "  batchsize = loader.shape[0]\n",
        "  \n",
        "  training_data   = np.empty([batchsize,22,4])\n",
        "  \n",
        "  train_label = np.empty((batchsize))\n",
        "  \n",
        " # print(\"start collecting training data\")\n",
        "  \n",
        "  for it,timestep_data in enumerate(loader):\n",
        "    train3d = np.hstack((pos,timestep_data[0:22].reshape([22,1])))\n",
        "    training_data[it] = train3d\n",
        "    train_label[it] = timestep_data[22] \n",
        " \n",
        "    \n",
        "  return (training_data, train_label)\n",
        "  \n",
        "  \n",
        "  \n",
        "def make_video(video_data):\n",
        "  return make_3d_data(video_data)\n",
        "  \n",
        "  \n",
        "  \n",
        "def get_training_data(train):\n",
        "\n",
        "  stream = np.zeros([train[0].shape[0],1,12,16,22])\n",
        "  \n",
        "  for i,image in enumerate(train[0]):\n",
        "    if i >= stream.shape[0]:\n",
        "      break\n",
        "    stream[i,0] = make_video(image)\n",
        "  \n",
        "  tensor = torch.from_numpy(stream)\n",
        "\n",
        "  min_v = np.amax(data)\n",
        "\n",
        "  max_v = np.amin(data)\n",
        "\n",
        "  limit = max(abs(min_v),abs(max_v))\n",
        "\n",
        "  tensor = tensor/limit\n",
        "  \n",
        "  return (tensor,train[1])\n",
        "\n",
        "def save_video(batch_size,path):\n",
        "  \n",
        "  !mkdir VideoBatches\n",
        "  \n",
        "  print(\"saving video files in VideoBatches with batch size: \",batch_size)\n",
        "  \n",
        "  train_loader, val_loader = get_data(batch_size)\n",
        "  \n",
        "  print(\"\\nsaving training data\\n\")\n",
        "  \n",
        "  for batch_idx, inputs in enumerate(train_loader):\n",
        "\n",
        "    if batch_idx <= 1309:\n",
        "      continue\n",
        "      \n",
        "    if batch_idx % 100 == 0:\n",
        "      print(\"saving batches: \" , batch_idx, \"-\", batch_idx+100)\n",
        "      \n",
        "    result_data = get_video_data(inputs)\n",
        "    result = get_training_data(result_data)\n",
        "    \n",
        "    name = \"VideoBatches/train_\" + str(batch_idx) + \".pickle\"\n",
        "    \n",
        "    with open(name, 'wb') as f:\n",
        "      pickle.dump([result[0], result[1]], f)\n",
        "      \n",
        "  print(\"\\nsaving validation data\\n\")\n",
        "  \n",
        "  for batch_idx, inputs in enumerate(train_loader):\n",
        "\n",
        "    if batch_idx % 100 == 0:\n",
        "      print(\"saving batches: \" , batch_idx, \"-\", batch_idx+100)\n",
        "      \n",
        "    result_data = get_video_data(inputs)\n",
        "    result = get_training_data(result_data)\n",
        "    \n",
        "    name = \"VideoBatches/val_\" + str(batch_idx) + \".pickle\"\n",
        "    \n",
        "    with open(name, 'wb') as f:\n",
        "      pickle.dump([result[0], result[1]], f)\n",
        "    \n",
        "    \n",
        "    \n",
        "  \n",
        "  \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZmEYzOGfeBk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#save_video(254,\"path\")\n",
        "#! rm -rf VideoBatches/*\n",
        "#!ls VideoBatches\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDuSahWOrvWF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install git+https://github.com/tsavchyn/coutils.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJZ6LnLZuXDS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from coutils import MyDrive\n",
        "\n",
        "drive = MyDrive(\"/drive\")\n",
        "drive.mount()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g75mGBWaupnT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from coutils import SaveToDrive\n",
        "\n",
        "# export = SaveToDrive()\n",
        "\n",
        "# saving the created data to the drive\n",
        "#for i in range(984,1312+1,1):\n",
        " # export.to_drive(\"VideoBatches/train_\"+str(i)+\".pickle\")\n",
        " # export.to_drive(\"VideoBatches/val_\"+str(i)+\".pickle\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtP2QmbcvfkI",
        "colab_type": "text"
      },
      "source": [
        "## CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bASv73CvfD5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class eeg_CNN(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(eeg_CNN, self).__init__()\n",
        "        self.T = 120\n",
        "        \n",
        "        self.conv1 = nn.Conv3d(1,5,2, stride=1,padding=(1,1,1),dilation=1)\n",
        "        self.batchnorm1 = nn.BatchNorm3d(5)\n",
        "        self.conv2 = nn.Conv3d(5,10,(2,3,2), stride=1,padding=(1,1,1),dilation=1)\n",
        "        self.batchnorm2 = nn.BatchNorm3d(10)\n",
        "        self.conv3 = nn.Conv3d(10,5,3, stride=1,dilation=1)\n",
        "        self.batchnorm3 = nn.BatchNorm3d(5)\n",
        "        self.conv4 = nn.Conv3d(5,5,(3,3,2), stride=1, dilation =1)\n",
        "        self.batchnorm4 = nn.BatchNorm3d(5)\n",
        "        self.conv5 = nn.Conv3d(5,5,(1,2,1), stride=1, dilation =1)\n",
        "        self.batchnorm5 = nn.BatchNorm3d(5)\n",
        "        self.pooling1 = nn.MaxPool3d((2,2,2), stride=1, dilation=1)\n",
        "        \n",
        "        #layer after max pool\n",
        "        self.conv6 = nn.Conv3d(5,5,3, stride=1,padding=(1,1,1),dilation=1)\n",
        "        self.batchnorm6 = nn.BatchNorm3d(5)\n",
        "        self.conv7 = nn.Conv3d(5,5,(3,3,2), stride=1,dilation=1)\n",
        "        self.batchnorm7 = nn.BatchNorm3d(5)\n",
        "        self.conv8 = nn.Conv3d(5,5,(1,2,1), stride=1,dilation=1)\n",
        "        self.batchnorm8 = nn.BatchNorm3d(5)\n",
        "        self.pooling2 = nn.MaxPool3d((3,3,2), stride=1, dilation=1)\n",
        "        \n",
        "        #Layer after next max pool \n",
        "        self.conv9 = nn.Conv3d(5,5,3, stride=1,padding=(1,1,1),dilation=1)\n",
        "        self.batchnorm9 = nn.BatchNorm3d(5)\n",
        "        self.conv10 = nn.Conv3d(5,5,(3,3,2), stride=1,dilation=1)\n",
        "        self.batchnorm10 = nn.BatchNorm3d(5)\n",
        "        self.conv11 = nn.Conv3d(5,1,(1,2,1), stride=1,dilation=1)\n",
        "        self.batchnorm11 = nn.BatchNorm3d(5)\n",
        "        self.pooling3 = nn.MaxPool3d((3,3,2), stride=1, dilation=1)\n",
        "        \n",
        "        self.fc1 = torch.nn.Linear(in_features=1 * 1 * 16, out_features=4)\n",
        "      \n",
        "\n",
        "        \n",
        "    def forward(self, x):\n",
        "\n",
        "        # first set of CNNs and then a max pool\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.batchnorm1(x)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.batchnorm2(x)\n",
        "        x = F.relu(self.conv3(x)) \n",
        "        x = self.batchnorm3(x)\n",
        "        x = F.relu(self.conv4(x))\n",
        "        x = self.batchnorm4(x)\n",
        "        x = F.relu(self.conv5(x))\n",
        "        x = self.batchnorm5(x)\n",
        "        x = self.pooling1(x)\n",
        "        \n",
        "        # second set\n",
        "        \n",
        "        x = F.relu(self.conv6(x))\n",
        "        x = self.batchnorm6(x)\n",
        "        x = F.relu(self.conv7(x))\n",
        "        x = self.batchnorm7(x)\n",
        "        x = F.relu(self.conv8(x))\n",
        "        x = self.batchnorm9(x)\n",
        "        x = self.pooling2(x)\n",
        "        \n",
        "        # set 3  \n",
        "        \n",
        "        x = F.relu(self.conv9(x))\n",
        "        x = self.batchnorm10(x)\n",
        "        x = F.relu(self.conv10(x))\n",
        "        x = self.batchnorm11(x)\n",
        "        x = F.relu(self.conv11(x))\n",
        "        x = self.pooling3(x)\n",
        "        \n",
        "        batch_size, timesteps, C, H, W = x.size()\n",
        "        \n",
        "        x = x.view(batch_size * timesteps, C, H, W)\n",
        "        \n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        \n",
        "        return F.softmax(x,dim=3)\n",
        "  \n",
        "def get_cost_function():\n",
        "    \n",
        "  cost_function = torch.nn.CrossEntropyLoss()\n",
        "  return cost_function\n",
        "    \n",
        "def get_optimizer(net, lr, wd, momentum):\n",
        "  #optimizer = torch.optim.SGD(net.parameters(), lr=lr, weight_decay=wd, momentum=momentum)\n",
        "  optimizer = torch.optim.Adam(net.parameters(), lr=lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=wd, amsgrad=False)\n",
        "  return optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CILv1uophoHm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(net, data_type, cost_function, device='cuda:0'):\n",
        "  samples = 0.\n",
        "  cumulative_loss = 0.\n",
        "  cumulative_accuracy = 0.\n",
        "\n",
        "  net.eval() # Strictly needed if network contains layers which has different behaviours between train and test\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    \n",
        "    for i in range(1312+1):\n",
        "\n",
        "      if i % 50 == 0:\n",
        "        print(\"on test batch: \" , i, \"-\", i+50)\n",
        "      \n",
        "      name = \"VideoBatches/\" + data_type + \"_\" + str(i) + \".pickle\"\n",
        "      \n",
        "      with open(name, 'rb') as f:\n",
        "        result = pickle.load(f)\n",
        "\n",
        "      \n",
        "      t_data   = result[0]\n",
        "      t_labels = result[1]\n",
        "      \n",
        "      # Load data into GPU\n",
        "      inputs = t_data.float().to(device)\n",
        "    \n",
        "      targets = torch.LongTensor(t_labels).to(device)\n",
        "        \n",
        "      # Forward pass\n",
        "      outputs = net(inputs).reshape([inputs.shape[0],4])\n",
        "\n",
        "      #print(outputs) \n",
        "      #print(targets)\n",
        "\n",
        "      # Apply the loss\n",
        "      loss = cost_function(outputs, targets)\n",
        "      #print(\"loss\",loss)\n",
        "\n",
        "      # Better print something\n",
        "      samples+=inputs.shape[0]\n",
        "      cumulative_loss += loss.item() # Note: the .item() is needed to extract scalars from tensors\n",
        "      _, predicted = outputs.max(1)\n",
        "      cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "      \n",
        "\n",
        "  return cumulative_loss/samples, cumulative_accuracy/samples*100\n",
        "\n",
        "\n",
        "def train(net,data_type,optimizer,cost_function, device='cuda:0'):\n",
        "  samples = 0.\n",
        "  cumulative_loss = 0.\n",
        "  cumulative_accuracy = 0.\n",
        "\n",
        "  \n",
        "  net.train() # Strictly needed if network contains layers which has different behaviours between train and test\n",
        "  \n",
        "  for i in range(1312+1):\n",
        "\n",
        "    if i % 50 == 0:\n",
        "      print(\"on train batch: \" , i, \"-\", i+50)\n",
        "      \n",
        "    name = \"VideoBatches/\" + data_type + \"_\" + str(i) + \".pickle\"\n",
        "      \n",
        "    with open(name, 'rb') as f:\n",
        "      result = pickle.load(f)\n",
        "\n",
        "      \n",
        "    t_data   = result[0]\n",
        "    t_labels = result[1]\n",
        "      \n",
        "    # Load data into GPU\n",
        "    inputs = t_data.float().to(device)\n",
        "    \n",
        "    targets = torch.LongTensor(t_labels).to(device)\n",
        "        \n",
        "    # Forward pass\n",
        "    outputs = net(inputs).reshape([inputs.shape[0],4])\n",
        "\n",
        "    # Apply the loss\n",
        "    loss = cost_function(outputs, targets)\n",
        "\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    \n",
        "    # Update parameters\n",
        "    optimizer.step()\n",
        "    \n",
        "    # Reset the optimizer\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Better print something, no?\n",
        "    samples+=inputs.shape[0]\n",
        "    cumulative_loss += loss.item()\n",
        "    _, predicted = outputs.max(1)\n",
        "    cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "  return cumulative_loss/samples, cumulative_accuracy/samples*100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1U9ZogaTimXY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main(batch_size=128, \n",
        "         device='cuda:0', \n",
        "         learning_rate=0.001, \n",
        "         weight_decay=0.000001, \n",
        "         momentum=0.9, \n",
        "         epochs=25):\n",
        "  \n",
        "  print(\"make data loaders\")\n",
        "  \n",
        "  train_loader = \"train\"\n",
        "  val_loader   = \"val\"\n",
        "  \n",
        "  print(\"network to device\")\n",
        "  net = eeg_CNN().to(device)\n",
        "  \n",
        "  print(\"optimizer\")\n",
        "  optimizer = get_optimizer(net, learning_rate, weight_decay, momentum)\n",
        "  \n",
        "  print(\"cost function\")\n",
        "  cost_function = get_cost_function()\n",
        "\n",
        "  print('Before training:')\n",
        "  train_loss, train_accuracy = test(net, train_loader, cost_function)\n",
        "  val_loss, val_accuracy     = test(net, val_loader, cost_function)\n",
        "  \n",
        "  print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "  print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "  print('-----------------------------------------------------')\n",
        "  \n",
        "  for e in range(epochs):\n",
        "    train_loss, train_accuracy = train(net, train_loader, optimizer, cost_function)\n",
        "    val_loss, val_accuracy = test(net, val_loader, cost_function)\n",
        "    print('Epoch: {:d}'.format(e+1))\n",
        "    print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "    print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "    print('-----------------------------------------------------')\n",
        "\n",
        "  print('After training:')\n",
        "  train_loss, train_accuracy = test(net, train_loader, cost_function)\n",
        "  val_loss, val_accuracy = test(net, val_loader, cost_function)\n",
        "  #test_loss, test_accuracy = test(net, test_loader, cost_function)\n",
        "\n",
        "  print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "  print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "  #print('\\t Test loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "  print('-----------------------------------------------------')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cAMMKKciyX1",
        "colab_type": "text"
      },
      "source": [
        "## Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poFRstlzluSP",
        "colab_type": "code",
        "outputId": "a150cba5-23b5-4097-c4e6-d90a434a66b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5763
        }
      },
      "source": [
        "main()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "make data loaders\n",
            "network to device\n",
            "optimizer\n",
            "cost function\n",
            "Before training:\n",
            "on test batch:  0 - 50\n",
            "on test batch:  50 - 100\n",
            "on test batch:  100 - 150\n",
            "on test batch:  150 - 200\n",
            "on test batch:  200 - 250\n",
            "on test batch:  250 - 300\n",
            "on test batch:  300 - 350\n",
            "on test batch:  350 - 400\n",
            "on test batch:  400 - 450\n",
            "on test batch:  450 - 500\n",
            "on test batch:  500 - 550\n",
            "on test batch:  550 - 600\n",
            "on test batch:  600 - 650\n",
            "on test batch:  650 - 700\n",
            "on test batch:  700 - 750\n",
            "on test batch:  800 - 850\n",
            "on test batch:  850 - 900\n",
            "on test batch:  900 - 950\n",
            "on test batch:  950 - 1000\n",
            "on test batch:  1000 - 1050\n",
            "on test batch:  1050 - 1100\n",
            "on test batch:  1100 - 1150\n",
            "on test batch:  1150 - 1200\n",
            "on test batch:  1200 - 1250\n",
            "on test batch:  1250 - 1300\n",
            "on test batch:  1300 - 1350\n",
            "on test batch:  0 - 50\n",
            "on test batch:  50 - 100\n",
            "on test batch:  100 - 150\n",
            "on test batch:  150 - 200\n",
            "on test batch:  200 - 250\n",
            "on test batch:  250 - 300\n",
            "on test batch:  300 - 350\n",
            "on test batch:  350 - 400\n",
            "on test batch:  400 - 450\n",
            "on test batch:  450 - 500\n",
            "on test batch:  500 - 550\n",
            "on test batch:  550 - 600\n",
            "on test batch:  600 - 650\n",
            "on test batch:  650 - 700\n",
            "on test batch:  700 - 750\n",
            "on test batch:  750 - 800\n",
            "on test batch:  800 - 850\n",
            "on test batch:  850 - 900\n",
            "on test batch:  900 - 950\n",
            "on test batch:  950 - 1000\n",
            "on test batch:  1000 - 1050\n",
            "on test batch:  1050 - 1100\n",
            "on test batch:  1100 - 1150\n",
            "on test batch:  1150 - 1200\n",
            "on test batch:  1200 - 1250\n",
            "on test batch:  1250 - 1300\n",
            "on test batch:  1300 - 1350\n",
            "\t Training loss 0.00543, Training accuracy 70.88\n",
            "\t Validation loss 0.00543, Validation accuracy 70.74\n",
            "-----------------------------------------------------\n",
            "on train batch:  0 - 50\n",
            "on train batch:  50 - 100\n",
            "on train batch:  100 - 150\n",
            "on train batch:  150 - 200\n",
            "on train batch:  200 - 250\n",
            "on train batch:  250 - 300\n",
            "on train batch:  300 - 350\n",
            "on train batch:  350 - 400\n",
            "on train batch:  400 - 450\n",
            "on train batch:  450 - 500\n",
            "on train batch:  500 - 550\n",
            "on train batch:  550 - 600\n",
            "on train batch:  600 - 650\n",
            "on train batch:  650 - 700\n",
            "on train batch:  700 - 750\n",
            "on train batch:  750 - 800\n",
            "on train batch:  800 - 850\n",
            "on train batch:  850 - 900\n",
            "on train batch:  900 - 950\n",
            "on train batch:  950 - 1000\n",
            "on train batch:  1000 - 1050\n",
            "on train batch:  1050 - 1100\n",
            "on train batch:  1100 - 1150\n",
            "on train batch:  1150 - 1200\n",
            "on train batch:  1200 - 1250\n",
            "on train batch:  1250 - 1300\n",
            "on train batch:  1300 - 1350\n",
            "on test batch:  0 - 50\n",
            "on test batch:  50 - 100\n",
            "on test batch:  100 - 150\n",
            "on test batch:  150 - 200\n",
            "on test batch:  200 - 250\n",
            "on test batch:  250 - 300\n",
            "on test batch:  300 - 350\n",
            "on test batch:  350 - 400\n",
            "on test batch:  400 - 450\n",
            "on test batch:  450 - 500\n",
            "on test batch:  500 - 550\n",
            "on test batch:  550 - 600\n",
            "on test batch:  600 - 650\n",
            "on test batch:  650 - 700\n",
            "on test batch:  700 - 750\n",
            "on test batch:  750 - 800\n",
            "on test batch:  800 - 850\n",
            "on test batch:  850 - 900\n",
            "on test batch:  900 - 950\n",
            "on test batch:  950 - 1000\n",
            "on test batch:  1000 - 1050\n",
            "on test batch:  1050 - 1100\n",
            "on test batch:  1100 - 1150\n",
            "on test batch:  1150 - 1200\n",
            "on test batch:  1200 - 1250\n",
            "on test batch:  1250 - 1300\n",
            "on test batch:  1300 - 1350\n",
            "Epoch: 1\n",
            "\t Training loss 0.00407, Training accuracy 71.27\n",
            "\t Validation loss 0.00407, Validation accuracy 71.27\n",
            "-----------------------------------------------------\n",
            "on train batch:  0 - 50\n",
            "on train batch:  50 - 100\n",
            "on train batch:  100 - 150\n",
            "on train batch:  150 - 200\n",
            "on train batch:  200 - 250\n",
            "on train batch:  250 - 300\n",
            "on train batch:  300 - 350\n",
            "on train batch:  350 - 400\n",
            "on train batch:  400 - 450\n",
            "on train batch:  450 - 500\n",
            "on train batch:  500 - 550\n",
            "on train batch:  550 - 600\n",
            "on train batch:  600 - 650\n",
            "on train batch:  650 - 700\n",
            "on train batch:  700 - 750\n",
            "on train batch:  750 - 800\n",
            "on train batch:  800 - 850\n",
            "on train batch:  850 - 900\n",
            "on train batch:  900 - 950\n",
            "on train batch:  950 - 1000\n",
            "on train batch:  1000 - 1050\n",
            "on train batch:  1050 - 1100\n",
            "on train batch:  1100 - 1150\n",
            "on train batch:  1150 - 1200\n",
            "on train batch:  1200 - 1250\n",
            "on train batch:  1250 - 1300\n",
            "on train batch:  1300 - 1350\n",
            "on test batch:  0 - 50\n",
            "on test batch:  50 - 100\n",
            "on test batch:  100 - 150\n",
            "on test batch:  150 - 200\n",
            "on test batch:  200 - 250\n",
            "on test batch:  250 - 300\n",
            "on test batch:  300 - 350\n",
            "on test batch:  350 - 400\n",
            "on test batch:  400 - 450\n",
            "on test batch:  450 - 500\n",
            "on test batch:  500 - 550\n",
            "on test batch:  550 - 600\n",
            "on test batch:  600 - 650\n",
            "on test batch:  650 - 700\n",
            "on test batch:  700 - 750\n",
            "on test batch:  750 - 800\n",
            "on test batch:  800 - 850\n",
            "on test batch:  850 - 900\n",
            "on test batch:  900 - 950\n",
            "on test batch:  950 - 1000\n",
            "on test batch:  1000 - 1050\n",
            "on test batch:  1050 - 1100\n",
            "on test batch:  1100 - 1150\n",
            "on test batch:  1150 - 1200\n",
            "on test batch:  1200 - 1250\n",
            "on test batch:  1250 - 1300\n",
            "on test batch:  1300 - 1350\n",
            "Epoch: 2\n",
            "\t Training loss 0.00403, Training accuracy 71.63\n",
            "\t Validation loss 0.00405, Validation accuracy 71.27\n",
            "-----------------------------------------------------\n",
            "on train batch:  0 - 50\n",
            "on train batch:  50 - 100\n",
            "on train batch:  100 - 150\n",
            "on train batch:  150 - 200\n",
            "on train batch:  200 - 250\n",
            "on train batch:  250 - 300\n",
            "on train batch:  300 - 350\n",
            "on train batch:  350 - 400\n",
            "on train batch:  400 - 450\n",
            "on train batch:  450 - 500\n",
            "on train batch:  500 - 550\n",
            "on train batch:  550 - 600\n",
            "on train batch:  600 - 650\n",
            "on train batch:  650 - 700\n",
            "on train batch:  700 - 750\n",
            "on train batch:  750 - 800\n",
            "on train batch:  800 - 850\n",
            "on train batch:  850 - 900\n",
            "on train batch:  900 - 950\n",
            "on train batch:  950 - 1000\n",
            "on train batch:  1000 - 1050\n",
            "on train batch:  1050 - 1100\n",
            "on train batch:  1100 - 1150\n",
            "on train batch:  1150 - 1200\n",
            "on train batch:  1200 - 1250\n",
            "on train batch:  1250 - 1300\n",
            "on train batch:  1300 - 1350\n",
            "on test batch:  0 - 50\n",
            "on test batch:  50 - 100\n",
            "on test batch:  100 - 150\n",
            "on test batch:  150 - 200\n",
            "on test batch:  200 - 250\n",
            "on test batch:  250 - 300\n",
            "on test batch:  300 - 350\n",
            "on test batch:  350 - 400\n",
            "on test batch:  400 - 450\n",
            "on test batch:  450 - 500\n",
            "on test batch:  500 - 550\n",
            "on test batch:  550 - 600\n",
            "on test batch:  600 - 650\n",
            "on test batch:  650 - 700\n",
            "on test batch:  700 - 750\n",
            "on test batch:  750 - 800\n",
            "on test batch:  800 - 850\n",
            "on test batch:  850 - 900\n",
            "on test batch:  900 - 950\n",
            "on test batch:  950 - 1000\n",
            "on test batch:  1000 - 1050\n",
            "on test batch:  1050 - 1100\n",
            "on test batch:  1100 - 1150\n",
            "on test batch:  1150 - 1200\n",
            "on test batch:  1200 - 1250\n",
            "on test batch:  1250 - 1300\n",
            "on test batch:  1300 - 1350\n",
            "Epoch: 3\n",
            "\t Training loss 0.00403, Training accuracy 71.78\n",
            "\t Validation loss 0.00403, Validation accuracy 71.66\n",
            "-----------------------------------------------------\n",
            "on train batch:  0 - 50\n",
            "on train batch:  50 - 100\n",
            "on train batch:  100 - 150\n",
            "on train batch:  150 - 200\n",
            "on train batch:  200 - 250\n",
            "on train batch:  250 - 300\n",
            "on train batch:  300 - 350\n",
            "on train batch:  350 - 400\n",
            "on train batch:  400 - 450\n",
            "on train batch:  450 - 500\n",
            "on train batch:  500 - 550\n",
            "on train batch:  550 - 600\n",
            "on train batch:  600 - 650\n",
            "on train batch:  650 - 700\n",
            "on train batch:  700 - 750\n",
            "on train batch:  750 - 800\n",
            "on train batch:  800 - 850\n",
            "on train batch:  850 - 900\n",
            "on train batch:  900 - 950\n",
            "on train batch:  950 - 1000\n",
            "on train batch:  1000 - 1050\n",
            "on train batch:  1050 - 1100\n",
            "on train batch:  1100 - 1150\n",
            "on train batch:  1150 - 1200\n",
            "on train batch:  1200 - 1250\n",
            "on train batch:  1250 - 1300\n",
            "on train batch:  1300 - 1350\n",
            "on test batch:  0 - 50\n",
            "on test batch:  50 - 100\n",
            "on test batch:  100 - 150\n",
            "on test batch:  150 - 200\n",
            "on test batch:  200 - 250\n",
            "on test batch:  250 - 300\n",
            "on test batch:  300 - 350\n",
            "on test batch:  350 - 400\n",
            "on test batch:  400 - 450\n",
            "on test batch:  450 - 500\n",
            "on test batch:  500 - 550\n",
            "on test batch:  550 - 600\n",
            "on test batch:  600 - 650\n",
            "on test batch:  650 - 700\n",
            "on test batch:  700 - 750\n",
            "on test batch:  750 - 800\n",
            "on test batch:  800 - 850\n",
            "on test batch:  850 - 900\n",
            "on test batch:  900 - 950\n",
            "on test batch:  950 - 1000\n",
            "on test batch:  1000 - 1050\n",
            "on test batch:  1050 - 1100\n",
            "on test batch:  1100 - 1150\n",
            "on test batch:  1150 - 1200\n",
            "on test batch:  1200 - 1250\n",
            "on test batch:  1250 - 1300\n",
            "on test batch:  1300 - 1350\n",
            "Epoch: 4\n",
            "\t Training loss 0.00401, Training accuracy 72.07\n",
            "\t Validation loss 0.00401, Validation accuracy 72.17\n",
            "-----------------------------------------------------\n",
            "on train batch:  0 - 50\n",
            "on train batch:  50 - 100\n",
            "on train batch:  100 - 150\n",
            "on train batch:  150 - 200\n",
            "on train batch:  200 - 250\n",
            "on train batch:  250 - 300\n",
            "on train batch:  300 - 350\n",
            "on train batch:  350 - 400\n",
            "on train batch:  400 - 450\n",
            "on train batch:  450 - 500\n",
            "on train batch:  500 - 550\n",
            "on train batch:  550 - 600\n",
            "on train batch:  600 - 650\n",
            "on train batch:  650 - 700\n",
            "on train batch:  700 - 750\n",
            "on train batch:  750 - 800\n",
            "on train batch:  800 - 850\n",
            "on train batch:  850 - 900\n",
            "on train batch:  900 - 950\n",
            "on train batch:  950 - 1000\n",
            "on train batch:  1000 - 1050\n",
            "on train batch:  1050 - 1100\n",
            "on train batch:  1100 - 1150\n",
            "on train batch:  1150 - 1200\n",
            "on train batch:  1200 - 1250\n",
            "on train batch:  1250 - 1300\n",
            "on train batch:  1300 - 1350\n",
            "on test batch:  0 - 50\n",
            "on test batch:  50 - 100\n",
            "on test batch:  100 - 150\n",
            "on test batch:  150 - 200\n",
            "on test batch:  200 - 250\n",
            "on test batch:  250 - 300\n",
            "on test batch:  300 - 350\n",
            "on test batch:  350 - 400\n",
            "on test batch:  400 - 450\n",
            "on test batch:  450 - 500\n",
            "on test batch:  500 - 550\n",
            "on test batch:  550 - 600\n",
            "on test batch:  600 - 650\n",
            "on test batch:  650 - 700\n",
            "on test batch:  700 - 750\n",
            "on test batch:  750 - 800\n",
            "on test batch:  800 - 850\n",
            "on test batch:  850 - 900\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_FTuCxxlxHz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E82xm2a5-TxF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}